{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "590f3205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ivershin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ivershin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ivershin/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ivershin/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ivershin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ivershin/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import scipy.spatial.distance as ds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk import wordnet as wn\n",
    "\n",
    "\n",
    "import wordsegment\n",
    "from wordsegment import load, segment\n",
    "load()\n",
    "\n",
    "import emoji\n",
    "import emojis\n",
    "\n",
    "import re\n",
    "import string\n",
    "from string import punctuation\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from preprocessing import clean_first, clean_second, split_word, count_of_smiles\n",
    "from preprocessing import penn2morphy, lem_pos, delete_stopwords, jaccard_similarity\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e204aac",
   "metadata": {},
   "source": [
    "# JSON parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dd82b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = [json.loads(line) for line in open('ranking_train.jsonl','r')]\n",
    "d_test = [json.loads(line) for line in open('ranking_test.jsonl','r')]\n",
    "\n",
    "comm = []\n",
    "rank = []\n",
    "uids = []\n",
    "uid = 0\n",
    "\n",
    "for post in d_train:\n",
    "    for com in post['comments']:\n",
    "        rank.append(com['score'])\n",
    "        comm.append(com['text'])\n",
    "        uids.append(uid)\n",
    "    \n",
    "    uid += 1\n",
    "    \n",
    "df_train = pd.DataFrame({'uid': uids, 'rank': rank, 'comm': comm})\n",
    "\n",
    "comm = []\n",
    "rank = []\n",
    "uids = []\n",
    "for post in d_test:\n",
    "    for com in post['comments']:\n",
    "        rank.append(com['score'])\n",
    "        comm.append(com['text'])\n",
    "        uids.append(uid)\n",
    "\n",
    "    uid += 1\n",
    "\n",
    "\n",
    "df_test = pd.DataFrame({'uid': uids, 'rank': rank, 'comm': comm})\n",
    "\n",
    "df_train['comm'] = df_train['comm'].apply(lambda x: clear_text(x))\n",
    "df_test['comm'] = df_test['comm'].apply(lambda x: clear_text(x))\n",
    "\n",
    "df_train['comm_?!_ratio'] = df_train['comm'].apply(lambda x: len([xx.start() for xx in re.finditer('[!?]', x)])/len(x))\n",
    "df_train['comm_tags'] = df_train['comm'].apply(lambda x: len([xx.start() for xx in re.finditer('#\\w+', x)]))\n",
    "df_train['comm_words'] = df_train['comm'].apply(lambda x: len(x.split()))\n",
    "df_test['comm_?!_ratio'] = df_test['comm'].apply(lambda x: len([xx.start() for xx in re.finditer('[!?]', x)])/len(x))\n",
    "df_test['comm_tags'] = df_test['comm'].apply(lambda x: len([xx.start() for xx in re.finditer('#\\w+', x)]))\n",
    "df_test['comm_words'] = df_test['comm'].apply(lambda x: len(x.split()))\n",
    "\n",
    "df_train.to_csv('ranking_train_comments.csv', index=False)\n",
    "df_test.to_csv('ranking_test_comments.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f5dcd5",
   "metadata": {},
   "source": [
    "# Prepare text for transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8c361f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ranking_train_comments.csv')\n",
    "df_test = pd.read_csv('ranking_test_comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c3615a",
   "metadata": {},
   "source": [
    "#### –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–º–∞–π–ª–∏–∫–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7bf628",
   "metadata": {
    "id": "ByiFRN6FMTbi"
   },
   "source": [
    "–ó–∞–ø–∏—Å—å —Å–º–∞–π–ª–æ–≤ (–≤ –≤–∏–¥–µ –∫–∞—Ä—Ç–∏–Ω–æ–∫ –≤–Ω—É—Ç—Ä–∏ —Ç–µ–∫—Å—Ç–∞) –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ed9bc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_smiles = ['))', '((', ':D',  '=D', ':-D', ':-(', ':-)', ':)','=)', ':(', '–æ_–û', 'o.O', ':-*', ':*', '^_^', ';)', ';-)', ':p'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63e9fcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['have_emoji'] = df['comm'].apply(lambda text: list(emojis.get(text)))\n",
    "df_test['have_emoji'] = df_test['comm'].apply(lambda text: list(emojis.get(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fa7a03",
   "metadata": {
    "id": "R-f4KrlhMqBS"
   },
   "source": [
    "–ó–∞–º–µ–Ω–∞ —Å–º–∞–π–ª–æ–≤ –Ω–∞ shortcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6cc93d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comm'] = df['comm'].apply(lambda x: emoji.demojize(x))\n",
    "df_test['comm'] = df_test['comm'].apply(lambda x: emoji.demojize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4db45e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TW0joTlG26sJ",
    "outputId": "8a827b98-a8cf-4405-a3ff-c973d8dc07ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                       []\n",
       "15009                                                  [‚ö´]\n",
       "15171                                                  [üòÆ]\n",
       "26818                                                  [üëç]\n",
       "33331    [2Ô∏è‚É£, 6Ô∏è‚É£, 8Ô∏è‚É£, 0Ô∏è‚É£, 5Ô∏è‚É£, 7Ô∏è‚É£, 3Ô∏è‚É£, 9Ô∏è‚É£, 1Ô∏è‚É£, ...\n",
       "56961                                                  [üòä]\n",
       "59879                                                  [üòâ]\n",
       "61699                                                  [üòÅ]\n",
       "Name: have_emoji, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.have_emoji.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ee4774",
   "metadata": {
    "id": "68lH_sxcM7Al"
   },
   "source": [
    "–ó–∞–º–µ–Ω–∞ –ª–∏—Å—Ç–∞ —Å–º–∞–π–ª–∏–∫–æ–≤ –Ω–∞ –∏—Ö –∫–æ–ª-–≤–æ  –≤ –ø—Ä–∏–∑–Ω–∞–∫–µ 'have_emoji'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "816bb71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['have_emoji'] = df['have_emoji'].apply(lambda smile: len(smile))\n",
    "df_test['have_emoji'] = df_test['have_emoji'].apply(lambda smile: len(smile))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608ac4f7",
   "metadata": {
    "id": "dzK34UqBMtAz"
   },
   "source": [
    "–ò—â–µ–º —Å–∏–º–≤–æ–ª—å–Ω—ã–µ —Å–º–∞–π–ª–∏–∫–∏, –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –∫–æ–ª-–≤–æ —Å–∏–º–≤–æ–ª—å–Ω—ã—Ö —Å–º–∞–π–ª–æ–≤ –≤ –ø—Ä–∏–∑–Ω–∞–∫ 'count_smiles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98d9bacf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df['count_smiles'] = df['comm'].apply(lambda smile: count_of_smiles(smile))\n",
    "df_test['count_smiles'] = df_test['comm'].apply(lambda smile: count_of_smiles(smile, list_of_smiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db117a9",
   "metadata": {},
   "source": [
    "–û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f414d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['count_smiles'] = df.apply(lambda smile: smile['have_emoji'] + smile['count_smiles'], axis=1)\n",
    "df_test['count_smiles'] = df_test.apply(lambda smile: smile['have_emoji'] + smile['count_smiles'], axis=1)\n",
    "df =  df.drop(columns=['have_emoji'], axis = 1)\n",
    "df_test = df_test.drop(columns=['have_emoji'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936e9757",
   "metadata": {},
   "source": [
    "#### –û—á–∏—â–∞–µ–º –æ—Ç –Ω–µ–∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤, –∑–∞–º–µ–Ω—è–µ–º —Å—Å—ã–ª–∫–∏ –∏ —Å–º–∞–π–ª–∏–∫–∏ –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb150ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comm']=df['comm'].apply(lambda x: x.encode(\"ascii\", \"ignore\").decode(\"utf-8\") )\n",
    "df['comm']=df['comm'].apply(lambda x: re.sub(r'\\<[^>]+\\>', '', x.lower()))\n",
    "df['comm']=df['comm'].apply(lambda x: re.sub(r'\\<[^>]*\\>', '', x.lower()))\n",
    "CLEANR = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "df['comm']=df['comm'].apply(lambda x: re.sub(CLEANR, '', x.lower()))\n",
    "df['comm']=df['comm'].apply(lambda x: re.sub(r'(https?://[^\\s]+)', 'URLURLURL', x.lower()))\n",
    "\n",
    "WEB_URL_REGEX = r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|~/.|https|.net|php|/.|[||]|config|biz|db|../|./|com|uk|co|info|ir|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?¬´¬ª‚Äú‚Äù‚Äò‚Äô])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\"\n",
    "df['comm']=df['comm'].apply(lambda x: re.sub(WEB_URL_REGEX, 'URLURLURL', x.lower()))\n",
    "\n",
    "df['comm']=df['comm'].apply(lambda x: re.sub(r'([;:]-*([()\\[\\]])\\2*)', 'SMILESMILE', x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdc92120",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['comm']=df_test['comm'].apply(lambda x: x.encode(\"ascii\", \"ignore\").decode(\"utf-8\") )\n",
    "df_test['comm']=df_test['comm'].apply(lambda x: re.sub(r'\\<[^>]+\\>', '', x.lower()))\n",
    "df_test['comm']=df_test['comm'].apply(lambda x: re.sub(r'\\<[^>]*\\>', '', x.lower()))\n",
    "CLEANR = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "df_test['comm']=df_test['comm'].apply(lambda x: re.sub(CLEANR, '', x.lower()))\n",
    "df_test['comm']=df_test['comm'].apply(lambda x: re.sub(r'(https?://[^\\s]+)', 'URLURLURL', x.lower()))\n",
    "\n",
    "WEB_URL_REGEX = r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|~/.|https|.net|php|/.|[||]|config|biz|db|../|./|com|uk|co|info|ir|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?¬´¬ª‚Äú‚Äù‚Äò‚Äô])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\"\n",
    "df_test['comm']=df_test['comm'].apply(lambda x: re.sub(WEB_URL_REGEX, 'URLURLURL', x.lower()))\n",
    "\n",
    "df_test['comm']=df_test['comm'].apply(lambda x: re.sub(r'([;:]-*([()\\[\\]])\\2*)', 'SMILESMILE', x.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd35a328",
   "metadata": {},
   "source": [
    "#### –°–æ–∑–¥–∞–µ–º —Ñ–∏—á—É, –µ—Å—Ç—å –ª–∏ –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ —Å–ª–æ–≤–æ, –Ω–∞–ø–∏—Å–∞–Ω–Ω–æ–µ –ö–ê–ü–°–û–ú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32e33ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['any_upper']=df['comm'].apply(lambda y: 1 if any([word.isupper() for word in y.split()]) else 0 )\n",
    "df_test['any_upper']=df_test['comm'].apply(lambda y: 1 if any([word.isupper() for word in y.split()]) else 0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625f07b6",
   "metadata": {},
   "source": [
    "#### –°–æ–∑–¥–∞–µ–º —Ñ–∏—á—É, –µ—Å—Ç—å –ª–∏ –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ —Ü–∏—Ñ—Ä–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5d84d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['any_digit']=df['comm'].apply(lambda y: 1 if any([word.isdigit() for word in y.split()]) else 0 )\n",
    "df_test['any_digit']=df_test['comm'].apply(lambda y: 1 if any([word.isdigit() for word in y.split()]) else 0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ba604",
   "metadata": {},
   "source": [
    "#### –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç \n",
    "–æ—Ç –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è —Å–∏–º–≤–æ–ª–æ–≤ –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ —Å–ª–æ–≤–∞ –≤ –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–Ω—É—é —Ñ–æ—Ä–º—É (isnt -> is not –∏ —Ç.–ø.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c2b1b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comm'] = df['comm'].fillna('+')\n",
    "df['comm'] =  [clean_first(r) for r in df['comm'].values]\n",
    "df['comm'] = df['comm'].fillna('+') \n",
    "\n",
    "df_test['comm'] = df_test['comm'].fillna('+')\n",
    "df_test['comm'] =  [clean_first(r) for r in df_test['comm'].values]\n",
    "df_test['comm'] = df_test['comm'].fillna('+') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c403c4cf",
   "metadata": {},
   "source": [
    "–ü—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –æ—Ç —Å–∏–º–≤–æ–ª–æ–≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –ø—É—Å—Ç—ã–º–∏ - –∑–∞–º–µ–Ω—è–µ–º —Ç–∞–∫–∏–µ —Å–ª—É—á–∞–∏ –Ω–∞ '+' (—á–∞—Å—Ç–æ —Å—Ç–∞–≤—è—Ç –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ö + –∫–∞–∫ —Å–æ–≥–ª–∞—Å–∏–µ —Å —á–µ–º-–ª–∏–±–æ, –ø–æ—ç—Ç–æ–º—É —Ç–∞–∫–∞—è –∑–∞–º–µ–Ω–∞ –∏–º–µ–µ—Ç —Å–º—ã—Å–ª)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "458646cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['post'] = df.groupby(\"uid\", group_keys=False)[\"post\"].apply(lambda x: x.ffill().bfill())\n",
    "\n",
    "df['post'] = df['post'].fillna('+')\n",
    "df['post'] =  [clean_first(r) for r in df['post'].values]\n",
    "df['post'] = df['post'].fillna('+') \n",
    "\n",
    "df_test['post'] = df_test.groupby(\"uid\", group_keys=False)[\"post\"].apply(lambda x: x.ffill().bfill())\n",
    "\n",
    "df_test['post'] = df_test['post'].fillna('+')\n",
    "df_test['post'] =  [clean_first(r) for r in df_test['post'].values]\n",
    "df_test['post'] = df_test['post'].fillna('+') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a2cc82",
   "metadata": {},
   "source": [
    "–ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –≤—Å—ë, —á—Ç–æ —Å–æ–¥–µ—Ä–∂–∏—Ç / (–Ω–µ —É–¥–∞–ª—è–ª–∏ —Ä–∞–Ω–µ–µ) - —ç—Ç–æ —Å—Å—ã–ª–∫–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fb6fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['post']=df['post'].apply(lambda x: x.replace('/', 'urlurlurl'))\n",
    "df['comm']=df['comm'].apply(lambda x: x.replace('/', 'urlurlurl'))\n",
    "\n",
    "df_test['post']=df_test['post'].apply(lambda x: x.replace('/', 'urlurlurl'))\n",
    "df_test['comm']=df_test['comm'].apply(lambda x: x.replace('/', 'urlurlurl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fa7b6e",
   "metadata": {},
   "source": [
    "–£–¥–∞–ª—è–µ–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Å–∏–º–≤–æ–ª—ã, –¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã, —Ç–∞–±—É–ª—è—Ü–∏–∏, –∞ —Ç–∞–∫–∂–µ —Ä–∞–∑–ª–µ–ø–ª—è–µ–º —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ —Å–ª–∏–ø–ª–∏—Å—å —á–µ—Ä–µ–∑ —Ä–∞–∑–ª–∏—á–Ω—É—é –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é ('hello.word' -> 'hello word' –∏ —Ç.–ø.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e0d1573",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comm'] =  [clean_second(r) for r in df['comm'].values]\n",
    "df['comm'] = df['comm'].fillna('+') \n",
    "df['post'] =  [clean_second(r) for r in df['post'].values]\n",
    "df['post'] = df['post'].fillna('+') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de2be5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['comm'] =  [clean_second(r) for r in df_test['comm'].values]\n",
    "df_test['comm'] = df_test['comm'].fillna('+') \n",
    "df_test['post'] =  [clean_second(r) for r in df_test['post'].values]\n",
    "df_test['post'] = df_test['post'].fillna('+') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96afac6",
   "metadata": {},
   "source": [
    "–†–∞–∑–ª–µ–ø–ª—è–µ–º —Å–ª–æ–≤–∞ –±–µ–∑ –ø—Ä–æ–±–µ–ª–∞ (anytypeyouwants -> any type you wants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cd78d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=[]\n",
    "text = []\n",
    "for t in tqdm_notebook(df['comm'].values):\n",
    "    k=[]\n",
    "    for j in t.split():\n",
    "        if len(j) > 2 and j.isalpha():\n",
    "            k+=[i for i in segment(j)]\n",
    "        else:\n",
    "            k+=[str(j)]\n",
    "    text += [' '.join(k)]\n",
    "df['comm'] = text\n",
    "\n",
    "k=[]\n",
    "text = []\n",
    "for t in tqdm_notebook(df['post'].values):\n",
    "    k=[]\n",
    "    for j in t.split():\n",
    "        if len(j) > 2 and j.isalpha():\n",
    "            k+=[i for i in segment(j)]\n",
    "        else:\n",
    "            k+=[str(j)]\n",
    "    text += [' '.join(k)]\n",
    "df['post'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c91ac5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dfedbccf5274361b7029c0807c55fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70020 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e15262f6564749b88ddf7c582bdb6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70020 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k=[]\n",
    "text = []\n",
    "for t in tqdm_notebook(df_test['comm'].values):\n",
    "    k=[]\n",
    "    for j in t.split():\n",
    "        if len(j) > 2 and j.isalpha():\n",
    "            k+=[i for i in segment(j)]\n",
    "        else:\n",
    "            k+=[str(j)]\n",
    "    text += [' '.join(k)]\n",
    "df_test['comm'] = text\n",
    "\n",
    "k=[]\n",
    "text = []\n",
    "for t in tqdm_notebook(df_test['post'].values):\n",
    "    k=[]\n",
    "    for j in t.split():\n",
    "        if len(j) > 2 and j.isalpha():\n",
    "            k+=[i for i in segment(j)]\n",
    "        else:\n",
    "            k+=[str(j)]\n",
    "    text += [' '.join(k)]\n",
    "df_test['post'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34a2e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['post']=df['post'].apply(lambda x: x.replace(\"smile smile\", \"smilesmile\"))\n",
    "df['post']=df['post'].apply(lambda x: x.replace(\"url url url\", \"urlurlurl\"))\n",
    "df['comm']=df['comm'].apply(lambda x: x.replace(\"url url url\", \"urlurlurl\"))\n",
    "df['comm']=df['comm'].apply(lambda x: x.replace(\"smile smile\", \"smilesmile\"))\n",
    "\n",
    "df_test['post']=df_test['post'].apply(lambda x: x.replace(\"smile smile\", \"smilesmile\"))\n",
    "df_test['post']=df_test['post'].apply(lambda x: x.replace(\"url url url\", \"urlurlurl\"))\n",
    "df_test['comm']=df_test['comm'].apply(lambda x: x.replace(\"url url url\", \"urlurlurl\"))\n",
    "df_test['comm']=df_test['comm'].apply(lambda x: x.replace(\"smile smile\", \"smilesmile\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459cb707",
   "metadata": {},
   "source": [
    "–ë–æ–ª–µ–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Å—ã–ª–∫–∏, —Å–º–∞–π–ª—ã –∏ –∞–¥—Ä–µ—Å–∞ –ø–æ—á—Ç—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fdfc646b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeda026128aa424bb7dac300b9bad962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/440535 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c0d6a0c9d4474c95ab758c1232b0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/440535 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1629e19e17084ad9b7b39c9a7cae962b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/440535 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k=[]\n",
    "text3 = []\n",
    "for t in tqdm_notebook(df['comm']):\n",
    "    k=[]\n",
    "    if 'urlurlurl' in t :\n",
    "        for j in t.split():\n",
    "            if 'urlurlurl' not in j:\n",
    "                k+=[j]\n",
    "            else:\n",
    "                k+=['URL']\n",
    "        text3 += [' '.join(k)]\n",
    "        \n",
    "    else:\n",
    "        text3 += [t]\n",
    "df['comm']=text3\n",
    "\n",
    "k=[]\n",
    "text3 = []\n",
    "for t in tqdm_notebook(df['comm']):\n",
    "    k=[]\n",
    "    if 'smilesmile' in t :\n",
    "        for j in t.split():\n",
    "            if 'smilesmile' not in j:\n",
    "                k+=[j]\n",
    "            else:\n",
    "                k+=['SMILE']\n",
    "        text3 += [' '.join(k)]\n",
    "    else:\n",
    "        text3 += [t]\n",
    "df['comm']=text3\n",
    "\n",
    "k=[]\n",
    "text3 = []\n",
    "for t in tqdm_notebook(df['comm']):\n",
    "    k=[]\n",
    "    if 'mail.' in t :\n",
    "        for j in t.split():\n",
    "            if 'mail.' not in j:\n",
    "                k+=[j]\n",
    "            else:\n",
    "                k+=['MAIL']\n",
    "        text3 += [' '.join(k)]\n",
    "    else:\n",
    "        text3 += [t]\n",
    "df['comm']=text3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a81023c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69cfa9a31e9b40099f5968fb81bf3c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/440535 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f64b70f1bab4674b4cb8489f0b9fb35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/440535 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5a8c83056646b1bc3e95e84e36ea75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/440535 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k=[]\n",
    "text3 = []\n",
    "for t in tqdm_notebook(df['post']):\n",
    "    k=[]\n",
    "    if 'urlurlurl' in t :\n",
    "        for j in t.split():\n",
    "            if 'urlurlurl' not in j:\n",
    "                k+=[j]\n",
    "            else:\n",
    "                k+=['URL']\n",
    "        text3 += [' '.join(k)]\n",
    "    else:\n",
    "        text3 += [t]\n",
    "df['post']=text3\n",
    "\n",
    "k=[]\n",
    "text3 = []\n",
    "for t in tqdm_notebook(df['post']):\n",
    "    k=[]\n",
    "    if 'smilesmile' in t :\n",
    "        for j in t.split():\n",
    "            if 'smilesmile' not in j:\n",
    "                k+=[j]\n",
    "            else:\n",
    "                k+=['SMILE']\n",
    "        text3 += [' '.join(k)]\n",
    "    else:\n",
    "        text3 += [t]\n",
    "df['post']=text3\n",
    "\n",
    "k=[]\n",
    "text3 = []\n",
    "for t in tqdm_notebook(df['post']):\n",
    "    k=[]\n",
    "    if 'mail.' in t :\n",
    "        for j in t.split():\n",
    "            if 'mail.' not in j:\n",
    "                k+=[j]\n",
    "            else:\n",
    "                k+=['MAIL']\n",
    "        text3 += [' '.join(k)]\n",
    "    else:\n",
    "        text3 += [t]\n",
    "df['post']=text3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "480a22a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51fc37b25e0d449ab3b19d513fdccb5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70020 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f60730a2cf4e6aa55a7feb8e242399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70020 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4183a3d78948bc8b16a912b159a562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70020 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff33199a02c349b7b2d9b336b9aa6368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70020 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd41d900d9064529886c181d7ddc598c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70020 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1036974bb0c44ea589acdf6147d0df41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70020 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k=[]\n",
    "text3 = []\n",
    "for t in tqdm_notebook(df_test['comm']):\n",
    "    k=[]\n",
    "    if 'urlurlurl' in t :\n",
    "        for j in t.split():\n",
    "            if 'urlurlurl' not in j:\n",
    "                k+=[j]\n",
    "            else:\n",
    "                k+=['URL']\n",
    "        text3 += [' '.join(k)]\n",
    "        \n",
    "    else:\n",
    "        text3 += [t]\n",
    "df_test['comm']=text3\n",
    "\n",
    "k=[]\n",
    "text3 = []\n",
    "for t in tqdm_notebook(df_test['comm']):\n",
    "    k=[]\n",
    "    if 'smilesmile' in t :\n",
    "        for j in t.split():\n",
    "            if 'smilesmile' not in j:\n",
    "                k+=[j]\n",
    "            else:\n",
    "                k+=['SMILE']\n",
    "        text3 += [' '.join(k)]\n",
    "    else:\n",
    "        text3 += [t]\n",
    "df_test['comm']=text3\n",
    "\n",
    "k=[]\n",
    "text3 = []\n",
    "for t in tqdm_notebook(df_test['comm']):\n",
    "    k=[]\n",
    "    if 'mail.' in t :\n",
    "        for j in t.split():\n",
    "            if 'mail.' not in j:\n",
    "                k+=[j]\n",
    "            else:\n",
    "                k+=['MAIL']\n",
    "        text3 += [' '.join(k)]\n",
    "    else:\n",
    "        text3 += [t]\n",
    "df_test['comm']=text3\n",
    "\n",
    "k=[]\n",
    "text3 = []\n",
    "for t in tqdm_notebook(df_test['post']):\n",
    "    k=[]\n",
    "    if 'urlurlurl' in t :\n",
    "        for j in t.split():\n",
    "            if 'urlurlurl' not in j:\n",
    "                k+=[j]\n",
    "            else:\n",
    "                k+=['URL']\n",
    "        text3 += [' '.join(k)]\n",
    "    else:\n",
    "        text3 += [t]\n",
    "df_test['post']=text3\n",
    "\n",
    "k=[]\n",
    "text3 = []\n",
    "for t in tqdm_notebook(df_test['post']):\n",
    "    k=[]\n",
    "    if 'smilesmile' in t :\n",
    "        for j in t.split():\n",
    "            if 'smilesmile' not in j:\n",
    "                k+=[j]\n",
    "            else:\n",
    "                k+=['SMILE']\n",
    "        text3 += [' '.join(k)]\n",
    "    else:\n",
    "        text3 += [t]\n",
    "df_test['post']=text3\n",
    "\n",
    "k=[]\n",
    "text3 = []\n",
    "for t in tqdm_notebook(df_test['post']):\n",
    "    k=[]\n",
    "    if 'mail.' in t :\n",
    "        for j in t.split():\n",
    "            if 'mail.' not in j:\n",
    "                k+=[j]\n",
    "            else:\n",
    "                k+=['MAIL']\n",
    "        text3 += [' '.join(k)]\n",
    "    else:\n",
    "        text3 += [t]\n",
    "df_test['post']=text3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dc5ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9cc6c576",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('train_for_transformers.csv', index=False)\n",
    "df_test.to_csv('test_for_transformers.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a16223e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['i not still waiting for them to stabilize wifi on the i pad sith ios 8. their quality has really started to slip for me since 10. 9 on mac.',\n",
       "       'for those who upgraded, no need to do a restore. you can just option-click update and downgrade the os back to 8. 0.0. instructions: URL',\n",
       "       'upgraded shortly after it was released and suffered the consequences. just was able to restore back down to 8. 0i had to turn i notes sage off and back on again in order for apple to re-register my number.. until then i was unable to send to any existing contacts.',\n",
       "       'i think they were under a lot of pressure on the health kit front. that was one of their big flagship ios 8 features, they got all these app developers to integrate it, then ios 8 shipped and they had a showstopper bug and wouldnt release any apps using healthkit.',\n",
       "       'fix for those who already updated: URL',\n",
       "       'get it from your employer. its better and cheaper than what you can get in the market. you are allow e had to supplement this may or may not make sense, but your final coverage will be a lot cheaper if you start with employer insurance.',\n",
       "       'as someone who moved from the uk to us under almost i wouldent ical conditions, let me say this: if you have a job that offers health insurance 99 of the ti note you will be better off taking that health insurance rather than buying it directly from the exchanges. why? at least two reasons: 1. employers are in a stronger negotiating position as insurance companies really fight for the business (it is for multiple people, so even if each one is less profitable the entire account will be more profitable with less overhead, so they want it).2. the people buying directly from exchanges are higher risk greater cost. they are normally poorer, self employed (which includes a lot of people in physically demanding professions as well, who get injured often), and some are early retirees (pre-medicare age, but still older).plus the biggest reason is that almost all employers pay a of your health insurance premium. if you go with the exchange your e paying 100 (of a higher base) rather than 70 of a lower one. so unless your employer has si not ply terrible health insurance, it is almost never a better i would ea from a financial perspect i have to go private. if your e really para noi would about it, just take the employer health insurance, assuming they pay 30 and you pay 70 of the premium, take an equivalent amount out of your pay and save it (e. g. save 30 of the premium a month). since that is what you would have been paying privately anyway (100), now your e just using the employers cut to save for a rainy day. personally if it was up to me employer provi would ed health insurance should be banned in the us and everyone (asi would e from the us military) should be required to go out and get private coverage (e. g. car insurance). that way the prices come down, competition goes up, and certain demographics (unemployed, self-employed, early retirees, etc) are not unfairly punish e would. but i do not see that happening any ti note soon (or ever) because employer provi would ed health insurance is too heavily ingrained in us culture. nobody even discussed destroying that link during the obama care debates. so whatever.. for now, until things change, employer is the way to go.',\n",
       "       'i went to URL filled out some online forms and picked a plan. its like URL for healthcare sort by brandpricefeatures, add to cart, check out. the only difference is that whatever insurer you sign up with sends out their own bill afterwards rather than paying the website. i have been buying my own insurance since reaching adulthood. before the federal exchange, i typically found personal plans through URL though they have less selection now than before the exchange. i think some insurers in states with an exchange pulled out of the private comparison sites like this.',\n",
       "       'full ti note employed, health insurance through my employer. group coverage (i. e. employer-based coverage) is the best insurance you can get in the us because of a number of already-gi haven reasons, as well as the fact that they can not reject you for a preexisting condition. all private plans will exclude preexisting conditions, so if your e already one some medication or treatment, you will have to wait at least a year before your insurer will cover it. i was a freelancer on a private plan previously. private plans were kind of bullshit (and very expen si have), but i do not know if the aca has changed that. i would like to see canada-style single-payer health care in the us, but i think it will be a couple of decades before we do.',\n",
       "       'your options depend on where you li have. i signed up for covered california, which is subsi would ized insurance. my husband (non-us citizen, but perm resi wouldent) is part of my plan. this depends on what state you li have in. some states rejected the federal healthcare URL for employer contributions, it depends on the employer i offer my employees insurance. if they choose not to take it, i do not gi have them a pay bump the cost per employee is negligible, and insurance is not offered in lieu of higher pay here. the purpose of me buying insurance for employees is mostly to ensure they are healthy while working for us, and getting care so as not to sicken others while on the job.',\n",
       "       'what i do not understand is why drugs developed with public money (government grants and non-profit charities) are not automatically in the public domain.',\n",
       "       'to everyone sure this will fail because of the amount of money required, ask yourself this: if you told your mom, and your neighbor, and your mailman, and your boss about the occ ulus rift, or pebble, or the peachy printer, or zach braff s latest movie, and the opportunity to help make those products a reality by preordering them donating money to them, do you really think any sizable portion of those people would be interested enough to donate? of course not. most successful kick starter and in diego go campaigns have an extremely narrow market, the early adopters of technology. cancer casts a wi would er net by several orders of magnitude. theres no reason to think that, with a good viral marketing campaign, something like this could not raise several orders of magnitude more money than the most successful crowd funding campaigns so far.',\n",
       "       'people do not seem to realize how expen si have it is to bring a new pharmaceutical drug to market and why this approach wont work. a typical drug will cost between 1bn-2bn and take about a decade of work by the ti note it hits the URL has details for those interested) crowd funding at those sort of levels just is not feasible yet.',\n",
       "       'many years ago, i worked in a lab where i synthesized a variant of interferon using soli would phase pept i would e synthesis. it was all manual work, using pretty basic materials. its just a pept i would e after all. i finish e would at 11 pm. cleaned it up, walked across the street to probably one of the worlds most respected cancer hospitals. gave it to the md on staff, he sai would well try it now. right now. a lot of things go on when people are at life and death cross roads. lots of things out of the mainstream. when all hope is gone, some ti notes miracles can happen.',\n",
       "       'i think the neg ati have comments on here have got it ups i would e down. i research and treat cancer, and i think this is great. the fact that it costs billions to develop a drug is why we should support novel pathways of organising drug development, rather than disparaging URL may not really be obvious from the out si woulde, but there is a coming crisis in that we wont as a society be able to afford new cancer drugs. it is regularly reported that new drugs which are typically priced around us120k per year (for multiple years in some cases) are not cost effect i have. case in point: crizotonib. this is a drug that works very well at shrinking tumours and controlling disease in incurable lung cancer. it is targeted - it only works if your tumour has a particular gene fusion, which occurs in about 4 of lung cancer patients. it is well tolerated compared to chemotherapy. its the kind of personalised medicine we are hoping to achieve more broadly in the future. and the way it is priced is not cost effect i have 1. i can guarantee you that the coming surge in i not mu no therapies will be exorbitantly priced. and these are drugs we will be giving not to 4 of cancer patients, but 50 of advanced cancer patients in a gi haven tumour type. these problems are even worse in countries where the government pays for drugs, because they are far more restricted in what they can afford. patients just end up missing out. and if you think having cancer is bad enough, knowing there is a drug that might help and not being able to afford it is just heart URL good luck to them, i hope they can draw some money away from nonsense like crowd funding potato salad. 1 URL',\n",
       "       'i not not a physicist. i i no tagine that if i were, this would be exciting news (that is, an exciting possibility).i am a literary critic, though, and i have to say that its bittersweet news at best. black holes are so much a part of the space-age i notagination. the article mentions hollywood, but black holes are woven deeply into our everyday metaphors and have been for decades. they have stood among the most wondrous things in the uni have rse an awe-inspiring URL reminds me a bit of when the press reported that the catholic church had gotten ri would of li not bo (the actual case was a bit more complex, and involved some fairly weighty theology).then again, we all still talk of things being in li notbo. perhaps that next project will still be a black hole, even if it turns out that theres no such thing.',\n",
       "       'i not not at all acqua m noted with astrophysics but i remember seeing an article state that there was a different physicist who proved this same thing: URL quick google shows his response to this: URL response: hawking radiation is unproven. and particle physicists are not aware that even without any quantum mechanics, classical gravitation dictates that all gravitational collapse must be accompanied by radiation. URL and then grav collapse should naturally lead to radiation supported eco: URL as far as non -formation of ``event horizon is concerned, in contrast to the conjectures of the present yet unpublish e would paper i gave exact proof: URL URL never highlighted my research which is infinitely more accurate than the present paper whose authors are not even aware that question of hawking radiation would arise only if there would already be a black hole with an event horizon. therefore this paper is not self-consistent. but that does not matter: the lead author of this paper has cam bri would ge URL URL is glad to highlight an inconsistent and yet unpublish e would paper by ignoring my series of my exact and original papers on the same topic. i do not have any background in astrophysics, can anyone comment on dr. mitra s response and the relation of his work to the above research? the best discussion i can find on dr. mitra s work is on quora: URL not at all acqua m noted with astrophysics but i remember seeing an article state that there was a different physicist who proved this same thing: URL quick google shows his response to this: URL response: hawking radiation is unproven. and particle physicists are not aware that even without any quantum mechanics, classical gravitation dictates that all gravitational collapse must be accompanied by radiation. URL URL and then grav collapse should naturally lead to radiation supported eco: URL as far as non -formation of ``event horizon is concerned, in contrast to the conjectures of the present yet unpublish e would paper i gave exact proof: URL URL never highlighted my research which is infinitely more accurate than the present paper whose authors are not even aware that question of hawking radiation would arise only if there would already be a black hole with an event horizon. therefore this paper is not self-consistent. but that does not matter: the lead author of this paper has cam bri would ge URL URL is glad to highlight an inconsistent and yet unpublish e would paper by ignoring my series of my exact and original papers on the same topic. i do not have any background in astrophysics, can anyone comment on dr. mitra s response and the relation of his work to the above research? the best discussion i can find on dr. mitra s work is on quora: URL',\n",
       "       'this paper clai not s that black holes can not come into existence, which is subtly different from clai noting that they can not exist. in particular, black holes can exist as long as theyve always existed its just that new ones can not come into existence. may be the uni have rse just has a fixed number of singularities. i am not a physicist this just struck me as an inaccurate i not plication in the article.',\n",
       "       'interesting; if the paper finds general acceptance, itll be fun to watch a new flurry of activity. the fairly recent controversial concept of firewalls has reinvigorated that subfield of physics. whether black holes exist or not, theres something really dense at the center of our galaxy. its a challenge to come up with a consistent non-black-hole explanation for the observed motion of stars at the galactic center. URL here: URL',\n",
       "       'ethan siegel just posted a detailed response to this. URL',\n",
       "       'as someone that does not do ios development it boggles my mind that you guys have to build this feature into your apps.',\n",
       "       'apple developed swift to make developers more express i have and efficient yet they do not provi would e si not ple controls like this. can not tell you how many hours i wasted in the chat portion of my app trying to get a si not ple growing text input to work!',\n",
       "       'so i not making ios apps some ti notes, but have not i not plemented a chat feature. why would i want to use this rather than a ui table view where i append the newest message in the end of the ui table view every ti note (you would have to increase its length 1 for each new message, but hey that may be ok)?',\n",
       "       'inverted mode for displaying cells ups i woulde-down (using catransform) a necessary hack for some messaging apps (including ours) care to elaborate? sounds intriguingly crufty.',\n",
       "       'cool. i was just shopping for a new growing text view since hp growing text view seemed like it was not mam no tained and buggy on ios 8. i settled on URL nice, si notple, and does the trick. howe haver, i will definitely checkout slacktextviewcontroller. for people not in ios development: yes this is crazy that it is not built in. but our tool chain is better SMILE',\n",
       "       'according to URL it appears that this is because bash allows functions to be exported through environment variables into subprocesses, but the code to parse those function definitions seems to be the same used to parse regular commands (and thus execute them).edit: after a brief glance over the affected code, this might not be so easy to patch completely - the actual method where everything interesting starts to take place is initialize shell variables in variables. c and parse and execute () in builtinsevalstring. c, so parsing and execution happen together; this is necessary to i not plement the shell grammar and is part of what makes it so powerful, but it can also be a source of vulnerabilities if its not used carefully. i suppose one attempt at fixing this could be to separate out the function parsing code into its own function, one which can not ever cause execution of its input, and use that to parse function definitions in environment variables. this would be a pretty easy and elegant thing to do with a recurs i have-descent parser, but bash uses a lexyacc-generated one to consume an entire command at once.. howe haver, all in all i not not so sure this ability to export func defs is such a good i would ea - for king a subshell automatically inherits the functions in the p are not, and if its a shell that wasnt created in such a manner, if it needs function definitions it can read them itself from some other source. this feature also means environment variables can not start with the string () (and exactly the string () - even removing the space between those characters, e. g. (), does not trigger it) without causing an error in any subprocess - violating the usual assumption that environment variables can hold any arbitrary string. it might be a rare case, but its certainly a cause for surprise.',\n",
       "       'funny, this works even after bash fix upgrade env x=() (a)=\\\\ sh -c echo date; cat e from URL',\n",
       "       'it might still be an issue. the patches may not have done enough. env x=() (a)=\\\\ sh -c echo date; cat echohttps: twitter. comtavisostatus514887394294652929env x=() (a)=\\\\ bash -c echo echo vuln; (cat echo) == vuln echo still vulnerable SMILE',\n",
       "       'heres how to patch ubuntu 8. 04 or anything where you have to build bash from source: assume that your sources are in src cd src wget URL download all patches for i in (seq -f 03g 0 25); do wget URL done tar zxvf bash-4. 3.tar. gz cd bash-4. 3 apply all patches for i in (seq -f 03g 0 25);do patch -p0 . bash43-i; done build and install . configure make make install not sure if ubuntu 8. 04 with custom built bash will be upgradable to 10. 04??',\n",
       "       'theres some misunderstanding of how the one-liner works, so heres a writeup. you can break the one-liner into two lines to see what is happening. 1. hobbesmedia: export badvar=() :;; echo vulnerable 2. hobbesmedia: bash -c echo i am an innocent sub process in bash version 3. bash: warning: badvar: ignoring function definition attempt 4. bash: error i not porting function definition for `badvar 5. i am an innocent sub process in 4. 3.25 (1)-release 1. create a specially crafted environment variable. ok, its done. but, nothing has happened! 2. create an innocent sub process. bash in this case. during initialization.. 3. .. bash spots the specially formed variable (named badvar), prints a warning, 4. .. and app are not ly does not define the function at all? 5. but other than that, the child bash runs as expected. and now the same two input lines on and old bash: 1. hobbesmetal: export badvar=() :;; echo vulnerable 2. hobbesmetal: bash -c echo i am an innocent sub process in bash version 3. vulnerable 4. i am an innocent sub process in 4. 3.22 (1)-release 1. create a specially crafted environment variable. ok, its done. but, nothing has happened! 2. create an innocent sub process. bash in this case. during initialization.. 3. .. bash acci would en tally executes a snippet that was in si would e the variable named badvar?!4. but other than that, the child bash runs as expected. wow, i should update that machine. SMILE',\n",
       "       'every ti note i need to unlock developer settings in a recent and roi would phone i get a little nostalgic for the codes of old. if your e of that era you likely also remember.. 007 373 5963.. and.. justin bailey',\n",
       "       'my first thought was that it was a sequence for a mortal kombat fatality.',\n",
       "       'the contra code . . . youll never forget . . . popularized among north american players in the nes version of contra, for which it was also dubbed both the contra code and 30 li have s codehttps: en. wikipedia. org wiki konami code',\n",
       "       'a friend of mine worked on the netflix debugging tools for a majority of their players (tvs, blu-ray players, consoles, etc) and there are a number of remote combinations like that in si would e the players to bring up all sorts of goodies.',\n",
       "       'heh, its pretty close to the konami code, i wonder if that was the inspiration? URL',\n",
       "       'in case people find it interesting, this is very si not ilar to how one efficiently builds a search engine for high qps and update rate (lockless real ti note document index), though that example is slightly more involved than a hash map. the basic premise is that the entry point to your data structure (or internal pointers) can change over ti note. you do not dismantle older entry points pointers until all older readers release, though you do not have to wait for all concurrent readers to exit.',\n",
       "       'could someone explain all of the grounds?',\n",
       "       'i wrote the paper and dissertation this work was based on, and i not happy to answer any questions people might have.(really awesome to see a production i not plementation of this.)',\n",
       "       'for gi have me if this is st upi would, but this could work for i not plement a concurrent vm?',\n",
       "       'may be i not daft, but the growing and shrinking explained looks like how you would do it for any hash table, is that not how a normal one works? do normal hash tables freeze the world to change tables or something? looks to me like an rcu grace period (not sure what this is, sleep maybe?) is introduced to allow concurrent threads ti note to finish reads in between pointer changes.',\n",
       "       'i learned basic on a atari 800 i got for christmas because our bundle included the car tri wouldge. after about 6 months i managed to get my p are not s to buy me a tape dri have so i wouldnt have to retype programs completely out each ti note. i think my biggest day of my programming life was when i got an advanced book that show e had me what gosub was for.',\n",
       "       'i wrote an atari st emulator out of nostalgia called ston x myself when i started to use a unix box (sun ipc), i believe it was the first open source st emulator (94 or 95, also the first for unix with big little endian support, ran on linux, dec stations with ultrix, sun solaris etc.). it was somewhat special in that it supported nati have graphics capabilities, i. e. bigger resolutions than 640x400 using vdi if the host hardware supported them. had a bunch of bugs though that i never got round to fixing.. my memories of the st were fond: it was quite powerful yet still a system that you could look into every aspect of, you could single-step through the os in rom if you wanted and there werent dozens of strange tasks running all the ti note like today. when mint (unix-like os layer) was released, we could even play in the unix league using gcc, bash, tcsh, emacs.',\n",
       "       'an atari st was the first computer i ever owned. i sent my father to the shops to buy a c64 and he was convinced by a salesman that what i really wanted was an st. atari shortly after pulled out of australia and i found myself paying 15 a month for copies of st format from england. seeing screenshots of the os bring up some serious childhood flashbacks.',\n",
       "       'a dark shadow falls over me when i think about all the hours of work spent by atari st engineers to build something almost lost to oblivion. sigh. there are so oooo oooo many things people have built which are sitting there in oblivion now. one of the most awesome things of playing with turn of the century computers (like pdp 11s or vaxen) is that you can see the stuff they di would, in the contra m not s they had, and really understand how amazingly inefficient overpowered oses are today. when you have only one font, its fixed wi would not h and size, and it comes out of a rom you do not worry about typography for example.',\n",
       "       'sorry, but the amiga was wayyyy better.',\n",
       "       'is there a mirror of this anywhere? i can not get to it right now. edit: working again',\n",
       "       '666 pages.. coin ci wouldence? i do not think so.',\n",
       "       'for those interested, it looks like programming examples are in java. probably a good read for those who never quite get classes.',\n",
       "       'the first four chapters are written with the professor j teaching environment in mind (cf. page URL the later chapters, the reader is encouraged to explore the concepts within a full-fledged java or c development environment such as eclipse, intellij, visual studio, etc.',\n",
       "       'very excited to see how they project their vision (htdp, etc) onto oop in full details.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['comm'].values[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc8e4ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['going back to school is not with giving up. some founders go back to school and keep working on the startup while there. those do so much worse than the people who work on the startup that going back to school seems, in practice, not too far removed from a death sentence for a startup. off the top of my head, guess had about startups where the founders went back to school. it does not only happen with summer batches. founders from winter batches do it too. usually the reason is that the startup is not doing very well. that judgement depends a lot on how determined the founders are. one reason we now shy away from funding people still in school is that they often unconsciously want the startup to fail, because the of dropping out frightens them. a lot of startups look bad after months. someone whos out of school and has to make it work or get a job in a cubicle will say do not worry, well figure out how to make it successful.',\n",
       "       'there will invariably be those who do not see the success they set out for, and they fall back to their original path. thats why a founders commitment is so critical. when your one plan is to become a startup founder, regardless of school or any other factors, you will do your damndest to make something of value and succeed. its part doing it better than the next guy and wanting it more than the next guy.',\n",
       "       'for me school is a way to be connected to what is going on in the real world. i entered school thinking it is either school or entrepreneurship. but in last year my views have changed. i really think there is a path where you do just enough to stay in school all the while trying out new ventures. (may be the steve jobs route?) school is an awesome platform to launch your site off and get quick feedback - especially if your venture fits in with the social networking ecosystem.',\n",
       "       'i guess it really depends on how hungry you are and how much you believe in your product. only and still in school as of the moment, yet more than willing to leave school in order to initiate my start up. come to think of it, willing to travel half the world just to get advice and connections from yc.',\n",
       "       'i know poll ground to go back to school after getting y combinator funding',\n",
       "       'it will be curious to see where this heads in the long run. cbs is on a tear but will it fit their will they try and establish control, overall agenda. enjoyed URL for many years supporting through paypal donations each i expire.. itll be interesting.',\n",
       "       'does this mean that theres now a big-name company who will fight for the repeal of the recent streaming-music royalty hike?',\n",
       "       'also on bbc news: URL . nice to see a london-based co. hit the headlines.',\n",
       "       'i do not understand what they do that is worth m a year.',\n",
       "       'sold out too cheaply. their leadership position, they should have ask for at least m',\n",
       "       'i really hate it when people falsely that the responsibilities of a public company includes profits at every opportunity. thinking long-term and win-win is the truly responsible ceos job. costco is the google of discount warehouses. wal-mart is microsoft.',\n",
       "       'i love costco. for me, the best quote of the article was this: the traditional retailer will say: selling this for . i wonder whether i can get . or . we say: were selling it for . how do we get it down to ?',\n",
       "       'but mr. sine gal warned that if costco increased markups to or percent, the company might slip down a dangerous slope and lose discipline in costs and prices. cost cos strict attitude toward product markup reminds me of signals frequent assertion that often constr am not s are a good thing to be embraced.',\n",
       "       'while costco does treat its employees a little bit better, people normally ignore the fact that wal-mart benefits a huge amount of low-income families who shop there. costco in the other hand targets the poor families do not have the space neither the stable income required to shop wholesale.',\n",
       "       'like to know more about their do not advertise strategy. do upscale shoppers talk to their friends about the deal they got on heinz ketchup?',\n",
       "       'his real point is that something can be and powerful. in order to sound edgy he subverts the meaning of complex into something that it does not mean (powerful), and bases his entire argument on this sleight of hand. (leading to statements that, out of his twisted context, are totally bewildering like its capabilities are complex.) his argument about and complexity not being mutually is a matter of definition, and thus he undermines others by foisting his definition into their mouths and then screaming bloody murder when he deduces that changing the premises of someones argument does indeed damage its conclusions. among other things i think his comparison of stats between mongrel and fogbugz is fundamentally broken. mongrel is among other things free. beyond the intense need to brag about his projects success i can not see any reason for making that particular comparison.(beyond this point i rant about his usage of hangul for his example, feel free to skip rest of post.) his comprehension of the korean writing system, which is one of his powerful points, seems superficial at best. hangul is a language that was designed by a small group of advisors when the royalty realized most koreans were illiterate (not enough resources to spend learning chinese kanji). how the hell are we supposed to be surprised that something explicitly designed to be in fact, turned out to be some extra points can be deducted for the fact that they stopped using hangul for several hundred years until it was institutionalized by the strong nationalist movement post wwii and made the official writing system in an effort to establish a unique korean culture. additionally we can pause curiously while noting that the average korean still uses - chinese kanji in daily life, and that academic papers still use the kanji for particularly esoteric or complex concepts (although this is increasingly discouraged as it is as elitism). he cru cif ies joel and don for using examples to justify a point.. and then use a single example to prove his, along with unsubstantiated about linguistic researchers. the japanese katakana alphabet can represent many words very well.. until you try to do one with an l in it, and then it fails utterly. using one example wouldnt reveal that, and its the same reason i doubt his point about hangul. beyond technicalities, its ignorant to conflate the creation of one small group of six hundred years ago with the cultural preferences of modern korea. using something as an example never works out if you do not actually know about it.',\n",
       "       'so thinking, well, never heard of this bug tracker called mongrel so ill check it out... turns out mongrel is a web server and not a bug tracker. so, guessing zed never heard the one about the apple and the orange.. i read up until he started criticising the apache web server. now, i wont to be an expert, but guessing theres a reason beyong it being a big turd for the billion installations. oh, well. tip to writers: as long as you are anonymous, pour on the bile. who cares? but when got your name attache would, do not try to make every enemy you can.',\n",
       "       'never seen a single blog post wander in so many directions. fogbugz. the nuclear arms race. korea. the virtues of the korean writing system. japanese wwii atrocities. fogbugz. ruby. tourism in asia. open source. the irony could be mistaken for satire if it wasnt so unfunny. he rants about joel making up numbers, and then does the exact same thing. his general call for others to cite references to back up their are preceded and by his own bold, unsubstantiated',\n",
       "       'it was a rant on a personal blog. its there to vent his frustration, not make a digg-bait and fogbugz does suck.',\n",
       "       'in the eternal words of the geico caveman. what?',\n",
       "       'looks like someone has not read the mythical man-month.',\n",
       "       'any chances for recreating this in a telecommuting way? i would have loved to attend, but crossing the atlantic seemed too far (although now i almost regret that i not go anyway). may be a hour startup competition would do, too? not working on a startup together, but everybody on their own startup? i participated in the h game programming competition once, and it was a lot of fun. people would just exchange advice and in irc while the event was on, and it was a great sense of community.',\n",
       "       'you know, if they started with and all but three or four of them reality-show style, i would be totally sold on it. now thats something one of these yc clones should try.',\n",
       "       'and what you do this weekend? love to hear about something like this if it happens in nyc. why is nyc not more of a startup hub?',\n",
       "       'very interesting. i would join an event like this if it happens in sv. not for the equity, just for the fun of it.',\n",
       "       'i smell bullshit. note from the article: companies are having a hard recruiting and retaining young talent, and as a result are accommodating what would have once been extreme demands. .. so many younger workers have gained the advantage when it comes to negotiating the terms of a new job. and today, economists and sociologists see such homecomings as a rational response to exorbitant housing prices in big cities, and entry-level wages that do not cover living URL which is it? employers can not find employees, or the labor market is so oversaturated in big cities that entry level positions do not pay living wages? my guess is that every company only wants to hire the top of graduates. so competition for the best (ivy league, specialized skills) grows while joe state and cathy community go chill with the folks and work at walmart. edit: how do you make block quotes?',\n",
       "       'not a bad read, although i wonder what reality some of these writers in. if we look at startup participation URL entrepreneurism amongst, say, people under years of age, is the percentage really that high on a national scale? if even of - year olds are opting to run startups instead of getting a job, be surprised (although love some numbers!)',\n",
       "       'more than anything, i think this is about all the money in the baby boom generation gen ys p are nots. it is not so much that most of our peers are between a) startupb) real job but that theres another obvious option: c) wait around for something better (possibly doing zilch in the if mom and dads basement is available indefinitely and you have no family obligations (wife, there may not be any pressure to do much of anything. personally, i can not not having some kind of small business, but i think those of us who are entrepre nuria l are vastly outnumbered by those who mooch.',\n",
       "       'some even have their p are not s in the room for added help, and many respected companies are willing to engage p are not s in the hiring process if thats what the wants (!!!) wow. depending on the circumstances, i would escort the bitty baby right to the front door, or.. explain that nepotism is absolutely',\n",
       "       'one thing that bugs me about slap is that it does not seem that they have much external pressure to succeed.. humphrey thinks of the mba program he is now in as sort of a backup plan in case slap does not take off at the end of the summer. three out of four of the founders of slap are getting financial help from their p are nots. i think needing money, and knowing the consequences are having to get a real job are often factors that can help contribute to success.',\n",
       "       'ok these are out the door. i have a few more here if anybody would like them.',\n",
       "       's goraya at URL', 'MAIL comthanks.', 'MAIL comthanks.',\n",
       "       'kyle. bolton at gmail',\n",
       "       'forgotten how well written that was. its better than of the essays on the web. and yet it was just an ordinary email, not intended for publication.',\n",
       "       'perfection is achieved, not when there is nothing more to add, but when there is nothing left to take away. - antoine de sam not-exupery',\n",
       "       'to get use of lfsps, it would be for more people to learn that you can and its completely reasonable to interface several programming languages. you need to be comfortable with both languages to work on the interface, but i find it surprising we not practice this at school at URL explicate what the article touches: perhaps you really have to use java because of platform or workforce requirements, but it does not mean you have to use java. more experienced developers can build some of the low-level classes using c via jni. they can also set up beanshell or jython for user interface and test scripters.',\n",
       "       'i totally understand peoples compl am not s about c. i am never sure if they are aware of some libraries that may concerns: ::function allows you to easily interchange function pointers, memeber function pointers and function objects. ::lamda allows you to define transient functions at the call site. this makes the old stl way more useful. to be sure the syntax can be a bit funky.. ::bind pulls all the fun of ::lamda and ::function together. with only the tiniest bit of discipline ::shared ptrs mean you never have to worry about memory management again. ::any and q variants (from the qt libraries) help you manage dynamic typing. so an array of ::anys can hold ints, chars, strings and any type you wants all at the same everything mentioned is either in the standard or will be soon. really undergrad c is only the beginning. the stl and are almost a language unto themseleves. i never use raw arrays or naked pointers anymore and its made development way faster while mam no taining URL asked, why use a language where adaptors have to be built to get the functionality of lisp rather than lisp itself (or something like that.) i see his point but if the effort of getting lisp to work with URL c based code is greater than the pain of using an lfm, why not make the best of an lfm? has anyone had the same experience? or am i putting the ass in masses?',\n",
       "       'for (i = ; i n; i) odd example to use. its always annoying to re-type the same thing over and over. but i recently went through a mountain of my java code and replaced: for (object obj : list) with int size=list. size ();for (int index=;indexsize;index) definitely very long suffering! it took a few hours but what you get with abstractions is a loss of control over when you use the tighter loop, the compiler turns everything (roughly) into this: iterator it=list. iterator (); URL URL which calls new list itr ();for (it. has next ()) object next=it. next ();the abstraction has to do that because it does not know whether the list (really, the java. util. collection) is a linked list or an arraylist or a set. if your loop itself is in a tight loop, it is not hard to call it millions and millions of during program execution which makes millions and millions of unnecessary allocations and method calls. there is a loss of performance which, at the end of the day, is more than how much you enjoy your job. of course, other languages would allow you to specify an for a abstraction.',\n",
       "       'i guess one of the few people who has tried two screens and not found a benefit.. may be its because i run so few applications - just emacs and firefox most of the i try a two screen setup for a couple of weeks, with emacs on one, and firefox on the other. it just not do anything for me. either working in emacs, or looking at the results in firefox. i do not find i need to do both at the same perhaps doing it wrong?',\n",
       "       'i not look at the date, but when i saw the price listings, i was like, what!? i just bought a for . i certainly appreciate all the extra real estate, and yet i can not deny that i could find a use for more. plugging my old crt into my integrated card, but it produces so much heat compared to a flat panel.',\n",
       "       'i am a big believer in screens as well. i have a flat panel plugged into my laptop, and keep my project management URL constantly on it while i work so i can easily move between working on the laptop and knowing what to work on on the lcd. SMILE',\n",
       "       'bah. a is vastly superior to two screens. think about it: with two screens, the valuable screen real-estate right in front of you, where you want the most things, is instead occupied by inches of black plastic.',\n",
       "       'why is it posted here?',\n",
       "       ' days and  people would work much better than  days and  people.',\n",
       "       'they learned a lesson indeed: development is hard. this is all that will matter.',\n",
       "       'is anyone else confused by the premise of startup weekend? it sounds like a fun but the whole concept seems a little silly to me.',\n",
       "       'the java platform was selected URL nuff',\n",
       "       'this is the first heard of startupweekend. they seem to be off on some basic as other comments here concur. throwing more developers at a problem does not usually solve it. a weekend is barely enough to think about an thoroughly. but it does sound like fun though. a barcamp should take the to the illogical extreme and hold a startup-hour session. SMILE'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comm'].values[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae8ec1f",
   "metadata": {},
   "source": [
    "# BERT infer embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd68d033",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "0c1974e9de3f4f46826f1584d761b4e8",
      "a0c436d65467471892f605c3fd427698",
      "630c7e236ec14858adf1368aaef9259a",
      "e0b258fa67d6410aabb125631eec9c68",
      "ef503952bdf649e0818bc4c733725148",
      "32a69b56a3394d94b1d96b3bc4dfe4dd",
      "39d5c8d89ffe4e69b32a70fb8f7995df",
      "cbbc95280e2940ec9770833582bcc322",
      "46ed0c94ed134cd4a7af835609b9f615",
      "439c82fd8808407593e6e6b7f068f517",
      "3110bf97f5d648d192b2a5d75d9b4a01",
      "3bc415c956c845488773706fa39ea083",
      "3c50237949bc424790d427fdedcaf291",
      "78f8ca352f12478380a60e9b1d385756",
      "3023d84035d145118a33723c04f1a2be",
      "99dd6b1fa2c04d09b00029a053f373ff",
      "8c7516c64f0b4f12a26baa308a350344",
      "f0ae3a7d25e645e188ad7f8c9be7bc51",
      "0853d0590ece439b9a9d30976b2edaec",
      "0ce1c2307bb647ef8156537f36e368a7",
      "a9136d4799a2426a85f0931d8e921c5a",
      "909504317eae47f18fa788e7220630b3",
      "3e76ac6cad6942449117a717b31c2a95",
      "b4d03cd3caf24f65aa3f74e375b7e7ef",
      "c0e6ea792c7843978b1960391fd739ae",
      "3ed477ba00fa42f9b0ab7c008eac3813",
      "5e7c24a7292443a6b2cb3082fc829730",
      "bf264e3d03f244c0b22706e8bd6cf229",
      "e8f7e238022c4d528b848e9e178fc00a",
      "86e5d57290694f40807d66a25390bde8",
      "de8d0089d9774eecb7fcbad30e6d5bc3",
      "b5f7d783fc16409a8c2dcdea141830d6",
      "db1befd01cfa40d2b6385821b85333d4",
      "1ec649d7501b446aa91facf2c99291e7",
      "7bf0209fa40640d4a77223e9aba56b4e",
      "228857396002446cbe02cb811132e87f",
      "e3d3bcf9bea942e2b3ac59233334c590",
      "b9e14ea433a3450d80b0c9b5baf0fd5a",
      "4a1620380f2d4388b657617baf48bdff",
      "0179427155794964a48fee8f28d226ce",
      "3abb85f1f23f406485d34de2e55e284c",
      "72862f1298274872aeb2190caccebfa4",
      "8e98765b66d74f6c8debfc868268c99e",
      "adeee4933d004136b765df1598195c59"
     ]
    },
    "id": "6Oz3httGh_nZ",
    "outputId": "57cfd680-84c3-4ecd-d833-e124a2b6581c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "max_len = 96\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertModel.from_pretrained('bert-base-cased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  max_length = max_len\n",
    "                                  )\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13f884b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "twoIau_VZCZZ",
    "outputId": "be6095de-1d43-4e7c-f2e8-b3870eb151e7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  uid  rank                                               comm  \\\n",
      "0           0    0     0  going back to school is not with giving up. so...   \n",
      "1           1    0     1  there will invariably be those who do not see ...   \n",
      "2           2    0     2  for me school is a way to be connected to what...   \n",
      "3           3    0     3  i guess it really depends on how hungry you ar...   \n",
      "4           4    0     4  i know poll ground to go back to school after ...   \n",
      "\n",
      "                                                post  comm_words  comm_tags  \\\n",
      "0  how many summer y combinator fund decided not ...         181          0   \n",
      "1  how many summer y combinator fund decided not ...          76          0   \n",
      "2  how many summer y combinator fund decided not ...          92          0   \n",
      "3  how many summer y combinator fund decided not ...          64          0   \n",
      "4  how many summer y combinator fund decided not ...          14          0   \n",
      "\n",
      "   comm_?!_ratio  count_smiles  any_upper  any_digit  \n",
      "0       0.000000             0          0          1  \n",
      "1       0.000000             0          0          0  \n",
      "2       0.002045             0          0          0  \n",
      "3       0.000000             0          0          1  \n",
      "4       0.000000             0          0          0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('train_for_transformers.csv')\n",
    "print(df.head())\n",
    "df['comm'] = df['comm'].fillna('+')\n",
    "texts = df['comm'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6839a85",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gd0ceGAiAUCE",
    "outputId": "4c33427f-cb5e-475b-bff9-23c3f28a22d5",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd16f751",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = np.zeros(shape=(0, 768))\n",
    "embs_out = 'train_comm_embs.npy'\n",
    "\n",
    "beg = 0 #–Ω–æ–º–µ—Ä –ø—Ä–∏–º–µ—Ä–∞, —Å –∫–æ—Ç–æ—Ä–æ–≥–æ –Ω–∞—á–∏–Ω–∞–µ–º\n",
    "batch_size = 1000 \n",
    "for idx in tqdm(range(beg, len(texts), batch_size)):\n",
    "    batch = texts[idx : min(len(texts), idx+batch_size)]\n",
    "    \n",
    "    encoded = tokenizer.batch_encode_plus(batch,max_length=max_len, padding='max_length', truncation=True)\n",
    "  \n",
    "    encoded = {key:torch.LongTensor(value).to(device) for key, value in encoded.items()}\n",
    "    with torch.no_grad():        \n",
    "        outputs = model(**encoded)\n",
    "        \n",
    "    \n",
    "    embs_p = outputs.last_hidden_state.cpu().numpy()\n",
    "    embs_p = np.mean(embs_p, axis=1)\n",
    "    embs = np.concatenate((embs, embs_p), axis = 0)\n",
    "\n",
    "    np.save(embs_out, embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9570753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_out_compr = 'train_comm_embs_compress.npy'\n",
    "\n",
    "pca = PCA(n_components=104)\n",
    "embs_compressed = pca.fit_transform(embs)\n",
    "np.save(embs_out_compr, embs_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9d7995e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WJuPylvsfjnH",
    "outputId": "6013b42e-0e55-40b3-ed4c-66875db10743",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8021073502567867\n"
     ]
    }
   ],
   "source": [
    "print(sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d3617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertModel.from_pretrained('bert-base-cased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  max_length = max_len\n",
    "                                  )\n",
    "model.to(device)\n",
    "\n",
    "df['post'] = df['post'].fillna('+')\n",
    "texts = df['post'].tolist()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "embs = np.zeros(shape=(0, 768))\n",
    "embs_out = 'train_post_embs.npy'\n",
    "\n",
    "beg = 0 #–Ω–æ–º–µ—Ä –ø—Ä–∏–º–µ—Ä–∞, —Å –∫–æ—Ç–æ—Ä–æ–≥–æ –Ω–∞—á–∏–Ω–∞–µ–º\n",
    "batch_size = 1000  \n",
    "for idx in tqdm(range(beg, len(texts), batch_size)):\n",
    "    batch = texts[idx : min(len(texts), idx+batch_size)]\n",
    "    \n",
    "    # encoded = tokenizer(batch)\n",
    "    encoded = tokenizer.batch_encode_plus(batch,max_length=max_len, padding='max_length', truncation=True)\n",
    "  \n",
    "    encoded = {key:torch.LongTensor(value).to(device) for key, value in encoded.items()}\n",
    "    with torch.no_grad():        \n",
    "        outputs = model(**encoded)\n",
    "        \n",
    "    \n",
    "    embs_p = outputs.last_hidden_state.cpu().numpy()\n",
    "    embs_p = np.mean(embs_p, axis=1)\n",
    "    embs = np.concatenate((embs, embs_p), axis = 0)\n",
    "\n",
    "    np.save(embs_out, embs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d779808",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6A0mmgWAeaEF",
    "outputId": "dae9a5fc-14d1-4a3a-fe06-5e8a7e045bbc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(440535, 104)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs_out_compr = 'train_post_embs_compress.npy'\n",
    "\n",
    "pca = PCA(n_components=104)\n",
    "embs_compressed = pca.fit_transform(embs)\n",
    "np.save(embs_out_compr, embs_compressed)\n",
    "embs_compressed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da0f6ec5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WJuPylvsfjnH",
    "outputId": "6013b42e-0e55-40b3-ed4c-66875db10743"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7227631450520113\n"
     ]
    }
   ],
   "source": [
    "print(sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443d9b35",
   "metadata": {},
   "source": [
    "# Prepare text for TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139d994b",
   "metadata": {},
   "source": [
    "#### –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ—á–∏—â–∞—Ç—å —Ç–µ–∫—Å—Ç –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb82afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('train_for_transformers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96954db7",
   "metadata": {},
   "source": [
    "–£–¥–∞–ª—è–µ–º —Ü–∏—Ñ—Ä—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6692c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comm']=df['comm'].fillna('+')\n",
    "df['comm']=df['comm'].apply(lambda x: re.sub(r'[0-9]+', '',x))\n",
    "df['post']=df['post'].fillna('+')\n",
    "df['post']=df['post'].apply(lambda x: re.sub(r'[0-9]+', '',x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f114f1c",
   "metadata": {},
   "source": [
    "–£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8b5faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comm']=df['comm'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "df['post']=df['post'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2425c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comm'] = [clean_third(r) for r in df['comm'].values]\n",
    "df['post'] = [clean_third(r) for r in df['post'].values]\n",
    "df['post']=df['post'].fillna('+')\n",
    "df['comm']=df['comm'].fillna('+')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d9c678",
   "metadata": {},
   "source": [
    "#### –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º POS-—Ç—ç–≥–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a69900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comm'] = [lem_pos(r) for r in tqdm_notebook(df['comm'].values)]\n",
    "df['post'] = [lem_pos(r) for r in tqdm_notebook(df['post'].values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc60119",
   "metadata": {},
   "source": [
    "#### –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bab519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = stopwords.words('english')\n",
    "\n",
    "stopwords_json = {\"en\":[\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]}\n",
    "stopwords_json_en = set(stopwords_json['en'])\n",
    "stopwords_nltk_en = set(stopwords.words('english'))\n",
    "stopwords_punct = set(punctuation)\n",
    "stoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en, stopwords_punct)\n",
    "\n",
    "df['comm']=df['comm'].apply(lambda text: delete_stopwords(text))\n",
    "df['comm']=df['comm'].fillna('+')\n",
    "df['post']=df['post'].apply(lambda text: delete_stopwords(text))\n",
    "df['post']=df['post'].fillna('+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fb7348",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('train_after_lem.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8d1881",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad459fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_after_lem.csv')\n",
    "texts = df['comm'].tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e48b711d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70535,)\n",
      "(17572,)\n"
     ]
    }
   ],
   "source": [
    "#–¥–µ–ª–∏–º –≤—ã–±–æ—Ä–∫–∏ –Ω–∞ train/test –ø–æ UID –ø–æ—Å—Ç–∞\n",
    "\n",
    "uids_uniq = np.array(list(set(df['uid'].tolist())))\n",
    "frac = 0.8\n",
    "uids_train = []\n",
    "uids_test = []\n",
    "\n",
    "for i in range(len(uids_uniq)):\n",
    "    if np.random.random()<=frac:\n",
    "        uids_train.append(uids_uniq[i])\n",
    "    else:\n",
    "        uids_test.append(uids_uniq[i])\n",
    "\n",
    "uids_train = np.array(uids_train)\n",
    "uids_test = np.array(uids_test)\n",
    "print(uids_train.shape)\n",
    "print(uids_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663ddfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('uids_train.npy', uids_train)\n",
    "np.save('uids_test.npy', uids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d48c607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "uids_train=np.load('uids_train.npy')\n",
    "uids_test=np.load('uids_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03f0e67",
   "metadata": {},
   "source": [
    "–¢–æ–ª—å–∫–æ —Å–ª–æ–≤–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6d89431",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_word1 = TfidfVectorizer(ngram_range=(1,1), sublinear_tf=True, max_features=10000)\n",
    "\n",
    "X_train, X_test = df.loc[df['uid'].isin(uids_train)]['comm'], df.loc[df['uid'].isin(uids_test)]['comm']\n",
    "\n",
    "train_vectors = tfidf_word1.fit_transform(X_train)\n",
    "test_vectors = tfidf_word1.transform(X_test)\n",
    "pca1 = PCA(n_components=104)\n",
    "embs_compressed_train = pca1.fit_transform(train_vectors.toarray())\n",
    "embs_compressed_test = pca1.transform(test_vectors.toarray())\n",
    "np.save(f'kfoldtfidf/1_1_train_vectors.npy',embs_compressed_train)\n",
    "np.save(f'kfoldtfidf/1_1_test_vectors.npy',embs_compressed_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf96222",
   "metadata": {},
   "source": [
    "–°–ª–æ–≤–∞ –∏ —Å–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ff5f490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_word2 = TfidfVectorizer(ngram_range=(1,2), sublinear_tf=True, max_features=10000)\n",
    "\n",
    "X_train, X_test = df.loc[df['uid'].isin(uids_train)]['comm'], df.loc[df['uid'].isin(uids_test)]['comm']\n",
    "\n",
    "train_vectors = tfidf_word2.fit_transform(X_train)\n",
    "test_vectors = tfidf_word2.transform(X_test)\n",
    "pca2 = PCA(n_components=104)\n",
    "embs_compressed_train = pca2.fit_transform(train_vectors.toarray())\n",
    "embs_compressed_test = pca2.transform(test_vectors.toarray())\n",
    "np.save(f'kfoldtfidf/2_1_train_vectors.npy',embs_compressed_train)\n",
    "np.save(f'kfoldtfidf/2_1_test_vectors.npy',embs_compressed_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fad133",
   "metadata": {},
   "source": [
    "# Extract more features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fe848a",
   "metadata": {},
   "source": [
    "#### –°—Ö–æ–∂–µ—Å—Ç—å –∫–æ–º–º–µ–Ω—Ç–∞ —Å –ø–æ—Å—Ç–æ–º (—Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78b92bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('train_after_lem.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5d4bc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['similar_with_Jaccard'] = df.apply(lambda x: jaccard_similarity(df['comm'], df['post']), axis=1)\n",
    "df['similar_with_Jaccard2_1']=df['similar_with_Jaccard']*df['similar_with_Jaccard']*(1-df['similar_with_Jaccard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a9d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comm_embs=np.load('train_comm_embs.npy')\n",
    "train_post_embs=np.load('train_post_embs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f1f09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_p_c = []\n",
    "for i in tqdm_notebook(range(0,len(train_comm_embs))):\n",
    "    cos_p_c.append(cosine_similarity((train_comm_embs[i],train_post_embs[i]))[1][0])\n",
    "df['post_comm']=cos_p_c\n",
    "df['post_comm2_1']=df['post_comm']*df['post_comm']*(1-df['post_comm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a7b7c3",
   "metadata": {},
   "source": [
    "#### –°—Ö–æ–∂–µ—Å—Ç—å –∫–æ–º–º–µ–Ω—Ç–∞ —Å –¥—Ä—É–≥–∏–º–∏ –∫–æ–º–º–µ–Ω—Ç–∞–º–∏ (—Å—Ä–µ–¥–Ω–µ–µ –∏ –º–∞–∫—Å–∏–º—É–º)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200ee2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_c_c_mean = []\n",
    "cos_c_c_max = []\n",
    "cos_c_c_min = []\n",
    "for i in tqdm_notebook(range(0,len(train_comm_embs))):\n",
    "    uid= df.iloc[i]['uid']\n",
    "    ind = df[df['uid']==uid].index.tolist()\n",
    "    ind.remove(i)\n",
    "    list_cos = []\n",
    "    for j in ind:\n",
    "        list_cos.append(cosine_similarity((train_comm_embs[j],train_comm_embs[i]))[1][0])    \n",
    "    cos_c_c_mean.append(np.mean(list_cos)) \n",
    "    cos_c_c_max.append(max(list_cos))\n",
    "    cos_c_c_min.append(min(list_cos))\n",
    "df['comm_comm_mean']=cos_c_c_mean\n",
    "df['comm_comm_max']=cos_c_c_max\n",
    "df['comm_comm_min']=cos_c_c_min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f994d8",
   "metadata": {},
   "source": [
    "#### –°—á–∏—Ç–∞–µ–º sentiment –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è –∏ —Å–º–æ—Ç—Ä–∏–º, —Å–æ–≤–ø–∞–¥–∞–µ—Ç –ª–∏ –µ–≥–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å –ø–æ—Å—Ç–æ–º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836a7326",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "df['sent_score']=df['comm'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "post_sent_score=df['post'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "df['post_and_comm_sent']=(post_sent_score*df['sent_score'] > 0).astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aced24bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('train_after_lem.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add54c21",
   "metadata": {},
   "source": [
    "# –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥–ª—è test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fef9399",
   "metadata": {},
   "source": [
    "BERT EMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d82d93d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n",
      "     uid  rank                                               comm  \\\n",
      "0  88107   NaN  i not still waiting for them to stabilize wifi...   \n",
      "1  88107   NaN  for those who upgraded, no need to do a restor...   \n",
      "2  88107   NaN  upgraded shortly after it was released and suf...   \n",
      "3  88107   NaN  i think they were under a lot of pressure on t...   \n",
      "4  88107   NaN             fix for those who already updated: URL   \n",
      "\n",
      "                                                post  comm_words  comm_tags  \\\n",
      "0  ios 8. 0.1 released, broken on i phone 6 model...          27          1   \n",
      "1  ios 8. 0.1 released, broken on i phone 6 model...          23          4   \n",
      "2  ios 8. 0.1 released, broken on i phone 6 model...          47          0   \n",
      "3  ios 8. 0.1 released, broken on i phone 6 model...          49          1   \n",
      "4  ios 8. 0.1 released, broken on i phone 6 model...           7          3   \n",
      "\n",
      "   comm_?!_ratio  count_smiles  any_upper  any_digit  \n",
      "0            0.0             0          0          0  \n",
      "1            0.0             0          0          0  \n",
      "2            0.0             0          0          0  \n",
      "3            0.0             0          0          1  \n",
      "4            0.0             0          0          0  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 96\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertModel.from_pretrained('bert-base-cased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  max_length = max_len\n",
    "                                  )\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_test = pd.read_csv('test_for_transformers.csv')\n",
    "print(df_test.head())\n",
    "df_test['comm'] = df_test['comm'].fillna('+')\n",
    "texts = df_test['comm'].tolist()\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ad859496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1f7c78ae0b4cfbbcb1c6673a4c6f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embs = np.zeros(shape=(0, 768))\n",
    "embs_out = 'test_comm_embs.npy'\n",
    "\n",
    "beg = 0 #–Ω–æ–º–µ—Ä –ø—Ä–∏–º–µ—Ä–∞, —Å –∫–æ—Ç–æ—Ä–æ–≥–æ –Ω–∞—á–∏–Ω–∞–µ–º\n",
    "batch_size = 1000 \n",
    "for idx in tqdm(range(beg, len(texts), batch_size)):\n",
    "    batch = texts[idx : min(len(texts), idx+batch_size)]\n",
    "    \n",
    "    encoded = tokenizer.batch_encode_plus(batch,max_length=max_len, padding='max_length', truncation=True)\n",
    "  \n",
    "    encoded = {key:torch.LongTensor(value).to(device) for key, value in encoded.items()}\n",
    "    with torch.no_grad():        \n",
    "        outputs = model(**encoded)\n",
    "        \n",
    "    \n",
    "    embs_p = outputs.last_hidden_state.cpu().numpy()\n",
    "    embs_p = np.mean(embs_p, axis=1)\n",
    "    embs = np.concatenate((embs, embs_p), axis = 0)\n",
    "\n",
    "    np.save(embs_out, embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "73579c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comm_embs=np.load('train_comm_embs.npy')\n",
    "train_post_embs=np.load('train_post_embs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1dfa79b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_out_compr = 'test_comm_embs_compress.npy'\n",
    "\n",
    "pca = PCA(n_components=104)\n",
    "embs_compressed = pca.fit_transform(train_comm_embs)\n",
    "embs_compressed = pca.transform(embs)\n",
    "np.save(embs_out_compr, embs_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d04be0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     uid  rank                                               comm  \\\n",
      "0  88107   NaN  i not still waiting for them to stabilize wifi...   \n",
      "1  88107   NaN  for those who upgraded, no need to do a restor...   \n",
      "2  88107   NaN  upgraded shortly after it was released and suf...   \n",
      "3  88107   NaN  i think they were under a lot of pressure on t...   \n",
      "4  88107   NaN             fix for those who already updated: URL   \n",
      "\n",
      "                                                post  comm_words  comm_tags  \\\n",
      "0  ios 8. 0.1 released, broken on i phone 6 model...          27          1   \n",
      "1  ios 8. 0.1 released, broken on i phone 6 model...          23          4   \n",
      "2  ios 8. 0.1 released, broken on i phone 6 model...          47          0   \n",
      "3  ios 8. 0.1 released, broken on i phone 6 model...          49          1   \n",
      "4  ios 8. 0.1 released, broken on i phone 6 model...           7          3   \n",
      "\n",
      "   comm_?!_ratio  count_smiles  any_upper  any_digit  \n",
      "0            0.0             0          0          0  \n",
      "1            0.0             0          0          0  \n",
      "2            0.0             0          0          0  \n",
      "3            0.0             0          0          1  \n",
      "4            0.0             0          0          0  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697338edaf46490095cd8cfa2b96156c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertModel.from_pretrained('bert-base-cased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  max_length = max_len\n",
    "                                  )\n",
    "model.to(device)\n",
    "\n",
    "df_test = pd.read_csv('test_for_transformers.csv')\n",
    "print(df_test.head())\n",
    "df_test['post'] = df_test['post'].fillna('+')\n",
    "texts = df_test['post'].tolist()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "embs = np.zeros(shape=(0, 768))\n",
    "embs_out = 'test_post_embs.npy'\n",
    "\n",
    "beg = 0 #–Ω–æ–º–µ—Ä –ø—Ä–∏–º–µ—Ä–∞, —Å –∫–æ—Ç–æ—Ä–æ–≥–æ –Ω–∞—á–∏–Ω–∞–µ–º\n",
    "batch_size = 1000  \n",
    "for idx in tqdm(range(beg, len(texts), batch_size)):\n",
    "    batch = texts[idx : min(len(texts), idx+batch_size)]\n",
    "    \n",
    "    # encoded = tokenizer(batch)\n",
    "    encoded = tokenizer.batch_encode_plus(batch,max_length=max_len, padding='max_length', truncation=True)\n",
    "  \n",
    "    encoded = {key:torch.LongTensor(value).to(device) for key, value in encoded.items()}\n",
    "    with torch.no_grad():        \n",
    "        outputs = model(**encoded)\n",
    "        \n",
    "    \n",
    "    embs_p = outputs.last_hidden_state.cpu().numpy()\n",
    "    embs_p = np.mean(embs_p, axis=1)\n",
    "    embs = np.concatenate((embs, embs_p), axis = 0)\n",
    "\n",
    "    np.save(embs_out, embs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e360b2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_out_compr = 'test_post_embs_compress.npy'\n",
    "\n",
    "pca = PCA(n_components=104)\n",
    "embs_compressed = pca.fit_transform(train_post_embs)\n",
    "embs_compressed = pca.transform(embs)\n",
    "np.save(embs_out_compr, embs_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0e5a65",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2d4937e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.read_csv('test_for_transformers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3c810ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['comm']=df_test['comm'].fillna('+')\n",
    "df_test['comm']=df_test['comm'].apply(lambda x: re.sub(r'[0-9]+', '',x))\n",
    "df_test['post']=df_test['post'].fillna('+')\n",
    "df_test['post']=df_test['post'].apply(lambda x: re.sub(r'[0-9]+', '',x))\n",
    "\n",
    "df_test['comm']=df_test['comm'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "df_test['post']=df_test['post'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ea5d9622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8852836679f24f469adabe298ca40268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70020 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e22f76e36e451e8616288468e5f5ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70020 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_test['comm'] = [lem_pos(r) for r in tqdm_notebook(df_test['comm'].values)]\n",
    "df_test['post'] = [lem_pos(r) for r in tqdm_notebook(df_test['post'].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5fb84282",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = stopwords.words('english')\n",
    "\n",
    "stopwords_json = {\"en\":[\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]}\n",
    "stopwords_json_en = set(stopwords_json['en'])\n",
    "stopwords_nltk_en = set(stopwords.words('english'))\n",
    "stopwords_punct = set(punctuation)\n",
    "stoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en, stopwords_punct)\n",
    "\n",
    "df_test['comm']=df_test['comm'].apply(lambda text: delete_stopwords(text, stoplist_combined))\n",
    "df_test['comm']=df_test['comm'].fillna('+')\n",
    "df_test['post']=df_test['post'].apply(lambda text: delete_stopwords(text, stoplist_combined))\n",
    "df_test['post']=df_test['post'].fillna('+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1223b4e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['wait stabilize wifi pad sith io quality start slip mac',\n",
       "       'upgrade restore optionclick update downgrade back instruction url',\n",
       "       'upgrade shortly release suffer consequence restore back turn note sage back order apple reregister number unable send exist contact',\n",
       "       'lot pressure health kit front big flagship io feature app developer integrate ios ship showstopper bug wouldnt release apps healthkit',\n",
       "       'fix update url',\n",
       "       'employer good cheap market supplement make sense final coverage lot cheap start employer insurance',\n",
       "       'move uk wouldent ical condition job offer health insurance ti note good health insurance buy directly exchange reason employer strong negotiating position insurance company fight business multiple people profitable entire account profitable overhead people buy directly exchange high risk great cost poor employ include lot people physically demand profession injure early retiree premedicare age olderplus big reason employer pay health insurance premium exchange pay high base low employer si ply terrible health insurance good ea financial perspect private para noi employer health insurance assume pay pay premium equivalent amount pay save save premium month pay privately employer cut save rainy day personally employer provi ed health insurance ban asi military require private coverage car insurance price competition demographic unemployed selfemployed early retiree unfairly punish happen ti note employer provi ed health insurance heavily ingrain culture discuss destroy link obama care debate thing change employer',\n",
       "       'url fill online form pick plan url healthcare sort brandpricefeatures add cart check difference insurer sign sends bill pay website buy insurance reach adulthood federal exchange typically find personal plan url selection exchange insurer state exchange pull private comparison site',\n",
       "       'full ti note employ health insurance employer group coverage employerbased coverage insurance number alreadygi reason fact reject preexisting condition private plan exclude preexisting condition medication treatment wait year insurer cover freelancer private plan previously private plan kind bullshit expen si aca change canadastyle singlepayer health care couple decade',\n",
       "       'option depend li sign cover california subsi ized insurance husband nonus citizen perm resi wouldent part plan depend state li state reject federal healthcare url employer contribution depend employer offer employee insurance choose gi pay bump cost employee negligible insurance offer lieu high pay purpose buy insurance employee ensure healthy work care sicken job',\n",
       "       'understand drug develop public money government grant nonprofit charity automatically public domain',\n",
       "       'fail amount money require mom neighbor mailman bos occ ulus rift pebble peachy printer zach braff late movie opportunity make product reality preordering donate money sizable portion people interested donate successful kick starter diego campaign extremely narrow market early adopter technology cancer cast wi er net order magnitude reason good viral marketing campaign raise order magnitude money successful crowd funding campaign',\n",
       "       'people realize expen si bring pharmaceutical drug market approach wont work typical drug cost bnbn decade work ti note hit url detail interested crowd funding sort level feasible',\n",
       "       'year ago work lab synthesize variant interferon solo phase pept synthesis manual work pretty basic material pept finish pm clean walk street world respected cancer hospital give md staff sai lot thing people life death cross roads lot thing mainstream hope ti note miracle happen',\n",
       "       'neg ati comment ups research treat cancer great fact cost billion develop drug support pathway organise drug development disparage url obvious si woulde crisis wont society afford cancer drug regularly report drug typically price usk year multiple year case cost effect case point crizotonib drug work shrink tumour control disease incurable lung cancer target work tumour gene fusion occur lung cancer patient tolerate compare chemotherapy kind personalised medicine hop achieve broadly future price cost effect guarantee surge mu therapy exorbitantly price drug give cancer patient advanced cancer patient gi tumour type problem bad country government pay drug restricted afford patient end miss cancer bad knowing drug afford heart url good luck hope draw money nonsense crowd funding potato salad url',\n",
       "       'physicist tagine excite news exciting possibilityi literary critic bittersweet news black hole part spaceage notagination article mention hollywood black hole weave deeply everyday metaphor decade stand wondrous thing uni rse aweinspiring url remind bit press report catholic church ri li bo actual case bit complex involve fairly weighty theologythen talk thing li notbo project black hole turn thing',\n",
       "       'acqua note astrophysics remember article state physicist prove thing url quick google show response url response hawk radiation unproven particle physicist aware quantum mechanic classical gravitation dictate gravitational collapse accompany radiation url grav collapse naturally lead radiation support eco url formation event horizon concern contrast conjecture present unpublish paper give exact proof url url highlight research infinitely accurate present paper author aware question hawk radiation arise black hole event horizon paper selfconsistent matter lead author paper cam bri ge url url glad highlight inconsistent unpublish paper ignore series exact original paper topic background astrophysics comment dr mitra response relation work research discussion find dr mitra work quora url acqua note astrophysics remember article state physicist prove thing url quick google show response url response hawk radiation unproven particle physicist aware quantum mechanic classical gravitation dictate gravitational collapse accompany radiation url url grav collapse naturally lead radiation support eco url formation event horizon concern contrast conjecture present unpublish paper give exact proof url url highlight research infinitely accurate present paper author aware question hawk radiation arise black hole event horizon paper selfconsistent matter lead author paper cam bri ge url url glad highlight inconsistent unpublish paper ignore series exact original paper topic background astrophysics comment dr mitra response relation work research discussion find dr mitra work quora url',\n",
       "       'paper clai black hole existence subtly clai note exist black hole exist long theyve exist existence uni rse fixed number singularity physicist strike inaccurate plication article',\n",
       "       'interest paper find general acceptance itll fun watch flurry activity fairly recent controversial concept firewall reinvigorate subfield physic black hole exist dense center galaxy challenge consistent nonblackhole explanation observed motion star galactic center url url',\n",
       "       'ethan siegel post detailed response url',\n",
       "       'io development boggle mind guy build feature apps',\n",
       "       'apple develop swift make developer express efficient provi si ple control hour waste chat portion app si ple grow text input work',\n",
       "       'make io apps ti note plemented chat feature ui table view append message end ui table view ti note increase length message hey',\n",
       "       'inverted mode display cell ups wouldedown catransform hack messaging apps include care elaborate sound intriguingly crufty',\n",
       "       'cool shop grow text view hp grow text view mam tained buggy io settle url nice si notple trick howe haver checkout slacktextviewcontroller people io development crazy build tool chain smile',\n",
       "       'accord url bash function export environment variable subprocesses code parse function definition parse regular command execute themedit glance affected code easy patch completely actual method interesting start place initialize shell variable variable parse execute builtinsevalstring parse execution happen plement shell grammar part make powerful source vulnerability carefully suppose attempt fix separate function parse code function execution input parse function definition environment variable pretty easy elegant thing recurs havedescent parser bash lexyaccgenerated consume entire command howe haver ability export func defs good ea king subshell automatically inherit function shell wasnt create manner function definition read source feature environment variable start string string remove space character trigger error subprocess violate usual assumption environment variable hold arbitrary string rare case surprise',\n",
       "       'funny work bash fix upgrade env sh echo date cat url',\n",
       "       'issue patch env sh echo date cat echohttps twitter comtavisostatusenv bash echo echo vuln cat echo vuln echo vulnerable smile',\n",
       "       'heres patch ubuntu build bash source assume source src cd src wget url download patch seq wget url tar zxvf bash tar gz cd bash apply patch seq patch bashi build install configure make make install ubuntu custom build bash upgradable',\n",
       "       'misunderstanding oneliner work heres writeup break oneliner line happen hobbesmedia export badvar echo vulnerable hobbesmedia bash echo innocent process bash version bash warn badvar ignore function definition attempt bash error port function definition badvar innocent process release create specially craft environment variable happen create innocent process bash case initialization bash spot specially form variable badvar print warning app ly define function child bash run expect input line bash hobbesmetal export badvar echo vulnerable hobbesmetal bash echo innocent process bash version vulnerable innocent process release create specially craft environment variable happen create innocent process bash case initialization bash acci en tally execute snippet si variable badvar child bash run expect wow update machine smile',\n",
       "       'ti note unlock developer setting recent roi phone nostalgic code era remember justin bailey',\n",
       "       'thought sequence mortal kombat fatality',\n",
       "       'contra code youll forget popularize north american player nes version contra dub contra code li codehttps en wikipedia org wiki konami code',\n",
       "       'friend mine work netflix debugging tool majority player tvs bluray player console number remote combination si player bring sort goody',\n",
       "       'heh pretty close konami code inspiration url',\n",
       "       'case people find interest si ilar efficiently build search engine high qps update rate lockless real ti note document index slightly involved hash map basic premise entry point data structure internal pointer change ti note dismantle entry point pointer reader release wait concurrent reader exit',\n",
       "       'explain ground',\n",
       "       'write paper dissertation work base happy answer question people havereally awesome production plementation',\n",
       "       'gi st upi work plement concurrent vm',\n",
       "       'daft grow shrink explain hash table normal work normal hash table freeze world change table rcu grace period sleep introduce concurrent thread ti note finish read pointer change',\n",
       "       'learn basic atari christmas bundle include car tri wouldge month manage buy tape dri wouldnt retype program completely ti note big day program life advanced book show gosub',\n",
       "       'write atari st emulator nostalgia call ston start unix box sun ipc open source st emulator unix big endian support run linux dec station ultrix sun solaris special support nati graphic capability big resolution vdi host hardware support bunch bug round fix memory st fond powerful system aspect singlestep rom werent dozen strange task run ti note today mint unixlike layer release play unix league gcc bash tcsh emacs',\n",
       "       'atari st computer send father shop buy convince salesman st atari shortly pull australia find pay month copy st format england screenshots bring childhood flashback',\n",
       "       'dark shadow fall hour work spend atari st engineer build lose oblivion sigh oooo oooo thing people build sit oblivion awesome thing play turn century computer pdp vaxen stuff di contra understand amazingly inefficient overpower os today font fixed wi size rom worry typography',\n",
       "       'amiga wayyyy good', 'mirror edit work', 'page coin ci wouldence',\n",
       "       'interested program java good read class',\n",
       "       'chapter write professor teach environment mind cf page url late chapter reader encourage explore concept fullfledged java development environment eclipse intellij visual studio',\n",
       "       'excited project vision htdp oop full detail'], dtype=object)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['comm'].values[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "31a59fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('test_after_lem.csv',index=False)\n",
    "uids_train=np.load('uids_train.npy')\n",
    "uids_test=np.load('uids_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6941273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = tfidf_word1.transform(df_test['comm'])\n",
    "embs_compressed = pca1.transform(vectors.toarray())\n",
    "np.save(f'kfoldtfidf/1_1_vectors.npy',embs_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "62bf6468",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = tfidf_word2.transform(df_test['comm'])\n",
    "embs_compressed = pca2.transform(vectors.toarray())\n",
    "np.save(f'kfoldtfidf/1_2_vectors.npy',embs_compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1d1a7d",
   "metadata": {},
   "source": [
    "Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a1855966",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.read_csv('test_after_lem.csv')\n",
    "test_comm_embs=np.load('test_comm_embs.npy')\n",
    "test_post_embs=np.load('test_post_embs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "71aa07b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(comm, text):\n",
    "    #Find intersection of two sets\n",
    "    comm_words = set(nltk.word_tokenize(comm))\n",
    "    text_words = set(nltk.word_tokenize(text))\n",
    "    nominator = comm_words.intersection(text_words)\n",
    "\n",
    "    #Find union of two sets\n",
    "    denominator = comm_words.union(text_words)\n",
    "\n",
    "    #Take the ratio of sizes\n",
    "    similarity = len(nominator)/len(denominator)\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a586603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6430dc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['comm']=df_test['comm'].fillna('+')\n",
    "df_test['post']=df_test['post'].fillna('+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2f702be1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e99c36289042b38ac53a91c3d02fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70020 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "df_test['similar_with_Jaccard'] = df_test.progress_apply(lambda x: jaccard_similarity(x['comm'], x['post']), axis=1)\n",
    "df_test['similar_with_Jaccard2_1']=df_test['similar_with_Jaccard']*df_test['similar_with_Jaccard']*(1-df_test['similar_with_Jaccard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1b09d565",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4e4ed129044872b03f0e2dcaddf4ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70020 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cos_p_c = []\n",
    "for i in tqdm_notebook(range(0,len(test_comm_embs))):\n",
    "    cos_p_c.append(cosine_similarity((test_comm_embs[i],test_post_embs[i]))[1][0])\n",
    "df_test['post_comm']=cos_p_c\n",
    "df_test['post_comm2_1']=df_test['post_comm']*df_test['post_comm']*(1-df_test['post_comm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6c469cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff33b5b68b60479095fcffb4f31c8e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70020 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cos_c_c_mean = []\n",
    "cos_c_c_max = []\n",
    "cos_c_c_min = []\n",
    "for i in tqdm_notebook(range(0,len(test_comm_embs))):\n",
    "    uid= df_test.iloc[i]['uid']\n",
    "    ind = df_test[df_test['uid']==uid].index.tolist()\n",
    "    ind.remove(i)\n",
    "    list_cos = []\n",
    "    for j in ind:\n",
    "        list_cos.append(cosine_similarity((test_comm_embs[j],test_comm_embs[i]))[1][0])    \n",
    "    cos_c_c_mean.append(np.mean(list_cos)) \n",
    "    cos_c_c_max.append(max(list_cos))\n",
    "    cos_c_c_min.append(min(list_cos))\n",
    "df_test['comm_comm_mean']=cos_c_c_mean\n",
    "df_test['comm_comm_max']=cos_c_c_max\n",
    "df_test['comm_comm_min']=cos_c_c_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a184cb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "df_test['sent_score']=df_test['comm'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "df_test['post_sent_score']=df_test['post'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "df_test['post_and_comm_sent']=(df_test['post_sent_score']*df_test['sent_score'] > 0).astype('int64')\n",
    "df_test=df_test.drop('post_sent_score',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0556b8e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>rank</th>\n",
       "      <th>comm</th>\n",
       "      <th>post</th>\n",
       "      <th>comm_words</th>\n",
       "      <th>comm_tags</th>\n",
       "      <th>comm_?!_ratio</th>\n",
       "      <th>count_smiles</th>\n",
       "      <th>any_upper</th>\n",
       "      <th>any_digit</th>\n",
       "      <th>similar_with_Jaccard</th>\n",
       "      <th>similar_with_Jaccard2_1</th>\n",
       "      <th>post_comm</th>\n",
       "      <th>post_comm2_1</th>\n",
       "      <th>comm_comm_mean</th>\n",
       "      <th>comm_comm_max</th>\n",
       "      <th>comm_comm_min</th>\n",
       "      <th>sent_score</th>\n",
       "      <th>post_and_comm_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wait stabilize wifi pad sith io quality start ...</td>\n",
       "      <td>io release break phone model withdraw</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.004148</td>\n",
       "      <td>0.845085</td>\n",
       "      <td>0.110635</td>\n",
       "      <td>0.889150</td>\n",
       "      <td>0.946946</td>\n",
       "      <td>0.809826</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>upgrade restore optionclick update downgrade b...</td>\n",
       "      <td>io release break phone model withdraw</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.864109</td>\n",
       "      <td>0.101467</td>\n",
       "      <td>0.892799</td>\n",
       "      <td>0.921799</td>\n",
       "      <td>0.870004</td>\n",
       "      <td>0.2960</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>upgrade shortly release suffer consequence res...</td>\n",
       "      <td>io release break phone model withdraw</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.001808</td>\n",
       "      <td>0.852067</td>\n",
       "      <td>0.107402</td>\n",
       "      <td>0.907617</td>\n",
       "      <td>0.946946</td>\n",
       "      <td>0.840996</td>\n",
       "      <td>-0.2500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lot pressure health kit front big flagship io ...</td>\n",
       "      <td>io release break phone model withdraw</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.006366</td>\n",
       "      <td>0.838686</td>\n",
       "      <td>0.113467</td>\n",
       "      <td>0.878200</td>\n",
       "      <td>0.921834</td>\n",
       "      <td>0.800234</td>\n",
       "      <td>-0.2023</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fix update url</td>\n",
       "      <td>io release break phone model withdraw</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.820896</td>\n",
       "      <td>0.120693</td>\n",
       "      <td>0.838114</td>\n",
       "      <td>0.901399</td>\n",
       "      <td>0.800234</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70015</th>\n",
       "      <td>102110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>send lot email month email newsletter business...</td>\n",
       "      <td>gmail hate domain</td>\n",
       "      <td>152</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001117</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.001734</td>\n",
       "      <td>0.796243</td>\n",
       "      <td>0.129183</td>\n",
       "      <td>0.934971</td>\n",
       "      <td>0.947039</td>\n",
       "      <td>0.916609</td>\n",
       "      <td>-0.4404</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70016</th>\n",
       "      <td>102110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hit si ilar problem send automate internal ema...</td>\n",
       "      <td>gmail hate domain</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.862990</td>\n",
       "      <td>0.102039</td>\n",
       "      <td>0.921032</td>\n",
       "      <td>0.925867</td>\n",
       "      <td>0.916609</td>\n",
       "      <td>-0.6597</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70017</th>\n",
       "      <td>102110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bit pre sum pti inflammatory amount pure specu...</td>\n",
       "      <td>gmail hate domain</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.845314</td>\n",
       "      <td>0.110532</td>\n",
       "      <td>0.935584</td>\n",
       "      <td>0.947669</td>\n",
       "      <td>0.918152</td>\n",
       "      <td>-0.5423</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70018</th>\n",
       "      <td>102110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>domain url mail server host server hetzner sco...</td>\n",
       "      <td>gmail hate domain</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>0.814647</td>\n",
       "      <td>0.123009</td>\n",
       "      <td>0.941121</td>\n",
       "      <td>0.948388</td>\n",
       "      <td>0.923499</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70019</th>\n",
       "      <td>102110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>curious thinking move personal domain gmail su...</td>\n",
       "      <td>gmail hate domain</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.006366</td>\n",
       "      <td>0.840348</td>\n",
       "      <td>0.112744</td>\n",
       "      <td>0.941625</td>\n",
       "      <td>0.948388</td>\n",
       "      <td>0.925867</td>\n",
       "      <td>-0.0516</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70020 rows √ó 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          uid  rank                                               comm  \\\n",
       "0       88107   NaN  wait stabilize wifi pad sith io quality start ...   \n",
       "1       88107   NaN  upgrade restore optionclick update downgrade b...   \n",
       "2       88107   NaN  upgrade shortly release suffer consequence res...   \n",
       "3       88107   NaN  lot pressure health kit front big flagship io ...   \n",
       "4       88107   NaN                                     fix update url   \n",
       "...       ...   ...                                                ...   \n",
       "70015  102110   NaN  send lot email month email newsletter business...   \n",
       "70016  102110   NaN  hit si ilar problem send automate internal ema...   \n",
       "70017  102110   NaN  bit pre sum pti inflammatory amount pure specu...   \n",
       "70018  102110   NaN  domain url mail server host server hetzner sco...   \n",
       "70019  102110   NaN  curious thinking move personal domain gmail su...   \n",
       "\n",
       "                                        post  comm_words  comm_tags  \\\n",
       "0      io release break phone model withdraw          27          1   \n",
       "1      io release break phone model withdraw          23          4   \n",
       "2      io release break phone model withdraw          47          0   \n",
       "3      io release break phone model withdraw          49          1   \n",
       "4      io release break phone model withdraw           7          3   \n",
       "...                                      ...         ...        ...   \n",
       "70015                      gmail hate domain         152          4   \n",
       "70016                      gmail hate domain          31          0   \n",
       "70017                      gmail hate domain          57          1   \n",
       "70018                      gmail hate domain          70          1   \n",
       "70019                      gmail hate domain          56          1   \n",
       "\n",
       "       comm_?!_ratio  count_smiles  any_upper  any_digit  \\\n",
       "0           0.000000             0          0          0   \n",
       "1           0.000000             0          0          0   \n",
       "2           0.000000             0          0          0   \n",
       "3           0.000000             0          0          1   \n",
       "4           0.000000             0          0          0   \n",
       "...              ...           ...        ...        ...   \n",
       "70015       0.001117             0          0          0   \n",
       "70016       0.000000             0          0          0   \n",
       "70017       0.000000             0          0          0   \n",
       "70018       0.000000             0          0          0   \n",
       "70019       0.000000             0          0          0   \n",
       "\n",
       "       similar_with_Jaccard  similar_with_Jaccard2_1  post_comm  post_comm2_1  \\\n",
       "0                  0.066667                 0.004148   0.845085      0.110635   \n",
       "1                  0.000000                 0.000000   0.864109      0.101467   \n",
       "2                  0.043478                 0.001808   0.852067      0.107402   \n",
       "3                  0.083333                 0.006366   0.838686      0.113467   \n",
       "4                  0.000000                 0.000000   0.820896      0.120693   \n",
       "...                     ...                      ...        ...           ...   \n",
       "70015              0.042553                 0.001734   0.796243      0.129183   \n",
       "70016              0.000000                 0.000000   0.862990      0.102039   \n",
       "70017              0.000000                 0.000000   0.845314      0.110532   \n",
       "70018              0.030303                 0.000890   0.814647      0.123009   \n",
       "70019              0.083333                 0.006366   0.840348      0.112744   \n",
       "\n",
       "       comm_comm_mean  comm_comm_max  comm_comm_min  sent_score  \\\n",
       "0            0.889150       0.946946       0.809826      0.0000   \n",
       "1            0.892799       0.921799       0.870004      0.2960   \n",
       "2            0.907617       0.946946       0.840996     -0.2500   \n",
       "3            0.878200       0.921834       0.800234     -0.2023   \n",
       "4            0.838114       0.901399       0.800234      0.0000   \n",
       "...               ...            ...            ...         ...   \n",
       "70015        0.934971       0.947039       0.916609     -0.4404   \n",
       "70016        0.921032       0.925867       0.916609     -0.6597   \n",
       "70017        0.935584       0.947669       0.918152     -0.5423   \n",
       "70018        0.941121       0.948388       0.923499      0.5719   \n",
       "70019        0.941625       0.948388       0.925867     -0.0516   \n",
       "\n",
       "       post_and_comm_sent  \n",
       "0                       0  \n",
       "1                       0  \n",
       "2                       0  \n",
       "3                       0  \n",
       "4                       0  \n",
       "...                   ...  \n",
       "70015                   1  \n",
       "70016                   1  \n",
       "70017                   1  \n",
       "70018                   0  \n",
       "70019                   1  \n",
       "\n",
       "[70020 rows x 19 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a7165214",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('test_after_lem.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f47e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
